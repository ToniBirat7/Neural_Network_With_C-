{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd15eedf",
   "metadata": {},
   "source": [
    "## **Resources**\n",
    "\n",
    "### **Training Neural Network with C++**\n",
    "\n",
    "[Linkedin_Learning](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/the-many-applications-of-machine-learning?autoSkip=true&resume=false&u=42288921)\n",
    "\n",
    "### **Understanding Neural Network in Depth**\n",
    "\n",
    "[Essential_Idea_Of_Neural_Network](https://www.youtube.com/watch?v=CqOfi41LfDw)\n",
    "\n",
    "[How_CNN_Works_in_Depth](https://www.youtube.com/watch?v=JB8T_zN7ZC0)\n",
    "\n",
    "### **The Mathematics Behind Neural Network**\n",
    "\n",
    "[Maths_Behind_Neural_Network](https://www.youtube.com/watch?v=Ixl3nykKG9M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71513c55",
   "metadata": {},
   "source": [
    "# **Tomorrow**\n",
    "\n",
    "[OR_Gate](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/solution-logic-gates-with-perceptrons?resume=false&u=42288921)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61e01b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162004",
   "metadata": {},
   "source": [
    "## **Neural Network Implementation Note**\n",
    "\n",
    "- All values must be real numbers, not integers. We will use double point precision (e.g., 0.1, 0.2).\n",
    "\n",
    "- The weights and inputs may be implemented as `1-D` vectors. We will use the `std::vector<double>` type from the C++ Standard Library i.e. `vec(w)` and `vec(x)`.\n",
    "\n",
    "- This way, the sum may be calculated in one operation: `z = vec(w) * vec(x)`.\n",
    "\n",
    "- We will feed the weighted sum to the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85211873",
   "metadata": {},
   "source": [
    "## **Files and Their Meaning**\n",
    "\n",
    "**`.h` files**: These are header files in C++ that typically contain function declarations, class definitions, and macros. They are included in `.cpp` files to provide the necessary declarations for the functions and classes used in the implementation.\n",
    "\n",
    "**`.cpp` files**: These are source files in C++ that contain the actual implementation of the functions and classes declared in the corresponding header files. They are compiled to create the final executable program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74332d50",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33287",
   "metadata": {},
   "source": [
    "## **Built in Functions That We Will Use**\n",
    "\n",
    "`std::vector`: A dynamic array that can resize itself automatically when elements are added or removed.\n",
    "\n",
    "Syntax: `std::vector<Type> vec;`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::inner_product`: Computes the inner product of two ranges.\n",
    "\n",
    "Syntax: `std::inner_product(first1, last1, first2, init);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::generate`: Fills a range with values generated by a function.\n",
    "\n",
    "Syntax: `std::generate(first, last, generator);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::push_back`: Adds an element to the end of a vector.\n",
    "\n",
    "Syntax: `vec.push_back(value);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::resize`: Changes the size of a vector.\n",
    "\n",
    "Syntax: `vec.resize(new_size);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::exp`: Computes the exponential function.\n",
    "\n",
    "Syntax: `std::exp(x);`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5671d9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2128f",
   "metadata": {},
   "source": [
    "## **Neural Network into Action**\n",
    "\n",
    "We will write all the declarations in the header files and all the implementations in the source files. This will help us keep our code organized and modular.\n",
    "\n",
    "Our first task it to implement basic `Multi Layer Perceptron` class in C++.\n",
    "\n",
    "For that we are creating `MLP.h` and `MLP.cpp` files.\n",
    "\n",
    "### **`MLP.h`**\n",
    "\n",
    "```C++\n",
    "\n",
    "// Perceptron class\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  // Run the perceptron\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  // Set Custom Weights if needed\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  // Sigmoid Activation Function\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "```\n",
    "\n",
    "Here, `size_t` is used to represent the number of inputs to the perceptron, ensuring that the value is always non-negative. It is an `unsigned integer` type which store `8 Bytes` in 64 Bit System and `4 Bytes` in 32 Bit System.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we'll implement the `Perceptron` class in the `MLP.cpp`.\n",
    "\n",
    "### **`MLP.cpp`**\n",
    "\n",
    "Here, we will write the implementation of the `Perceptron` class.\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "```\n",
    "\n",
    "**Below is the Step wise Step Explanation for Each Implementation.**\n",
    "\n",
    "`weights.resize(inputs + 1);`\n",
    "\n",
    "This line resizes the weights vector to hold the specified number of inputs plus one `additional` element for the bias. This ensures that the weights vector has the correct size to accommodate all input weights and the bias term.\n",
    "\n",
    "`generate(weights.begin(), weights.end(), frand);`\n",
    "\n",
    "This line fills the weights vector with random values generated by the `frand` function. The `generate` function takes a range (from the beginning to the end of the weights vector) and applies the `frand` function to each element in that range, effectively initializing the weights to small random values.\n",
    "\n",
    "`x.push_back(bias);`\n",
    "\n",
    "This line adds the bias term to the end of the input vector `x`. This is necessary because the bias is treated as an additional input to the perceptron, and it needs to be included in the weighted sum calculation.\n",
    "\n",
    "`inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);`\n",
    "\n",
    "This line computes the weighted sum of the inputs by taking the inner product of the input vector `x` (which now includes the bias) and the weights vector. The `inner_product` function multiplies each element of the input vector by the corresponding element of the weights vector and sums the results. The last argument `(double)0.0` specifies the initial value for the sum.\n",
    "\n",
    "`return sigmoid(sum);`\n",
    "\n",
    "This line passes the computed weighted sum into the sigmoid function and returns the result. The sigmoid function applies the logistic activation function to the weighted sum, squashing the output to a range between 0 and 1. This is a crucial step in the perceptron's operation, as it determines the final output of the neuron.\n",
    "\n",
    "`weights = w_init;`\n",
    "\n",
    "This line sets the weights of the perceptron to the provided initialization vector `w_init`. This allows the user to specify custom weights for the perceptron, which can be useful for tasks like transfer learning or fine-tuning a pre-trained model.\n",
    "\n",
    "`return 1.0 / (1.0 + exp(-x));`\n",
    "\n",
    "Calculates the sigmoid activation value for the given input `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e343f1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7649416",
   "metadata": {},
   "source": [
    "## **AND Gate**\n",
    "\n",
    "Both the inputs need to be `True` for `True` output.\n",
    "\n",
    "Now how do we create a Perceptron that can classify inputs like an AND gate?\n",
    "\n",
    "Let's visualize the inputs and outputs of the `AND` gate in a Graph:\n",
    "\n",
    "<img src='./Notes_Images/and_gate.png'>\n",
    "\n",
    "Now, to successfully classify we need to draw a line that separates the two classes (0 and 1). This line is called the decision boundary.\n",
    "\n",
    "<img src='./Notes_Images/boundary.png'>\n",
    "\n",
    "**The Line that is To be Drawn is of Sigmoid Function**\n",
    "\n",
    "<img src='./Notes_Images/sigmoid.png'>\n",
    "\n",
    "In this image, the boundary is the line where sigmoid is `0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Before Moving Forward`,\n",
    "\n",
    "Let's try to implement a function that exactly mimics as `AND` gate, but the function should be linear i.e. `f(x1, x2) = w1*x1 + w2*x2 + b`\n",
    "\n",
    "**Is that Possible?**\n",
    "\n",
    "<img src='./Notes_Images/linear_and.png'>\n",
    "\n",
    "This proof shows that it is not possible to create a linear function that mimics the behavior of an `AND` gate.\n",
    "\n",
    "The only solution is that the function should be non-linear, which means the function can be `exponential`, `quadratic`, or any other non-linear form.\n",
    "\n",
    "<img src='./Notes_Images/non_linear_and.png'>\n",
    "\n",
    "**TL;DR: AND is `linearly separable` (a perceptron can classify it), but it is not a linear function of the `inputs`.**\n",
    "\n",
    "### **A Perceptron as an AND Gate**\n",
    "\n",
    "Let's say there are two inputs `x1` and `x2`. The perceptron will compute a weighted sum of the inputs and pass it through a step function to produce the output.\n",
    "\n",
    "The weighted sum can be represented as:\n",
    "\n",
    "```\n",
    "z = w1*x1 + w2*x2 + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `w1` and `w2` are the weights for the inputs\n",
    "- `b` is the bias term\n",
    "\n",
    "<hr>\n",
    "\n",
    "The earlier problem was that we were not able to find a linear function that could separate the two classes (0 and 1).\n",
    "\n",
    "But if we pass the output of the `linear function` i.e. `z = w1*x1 + w2*x2 + b` through a `non-linear activation` function i.e. `sigmoid`, we can achieve the desired results.\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "```\n",
    "Ïƒ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where `e` is the base of the natural logarithm.\n",
    "\n",
    "When the value of `z` is `0`, the sigmoid function outputs `0.5`.\n",
    "\n",
    "When `z` is positive, the sigmoid function outputs a value between `0.5` and `1`. When `z` is negative, the sigmoid function outputs a value between `0` and `0.5`.\n",
    "\n",
    "For positive value, the output converges to `1` as `z` increases. For negative value, the output converges to `0` as `z` decreases.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The step function will output `1` if `z` is greater than or equal to `0`, and `0` otherwise.\n",
    "\n",
    "To implement the AND gate, we need to find appropriate values for `w1`, `w2`, and `b` such that the perceptron produces the correct output for all possible combinations of inputs.\n",
    "\n",
    "The truth table for the AND gate is as follows:\n",
    "\n",
    "| x1  | x2  | AND |\n",
    "| --- | --- | --- |\n",
    "| 0   | 0   | 0   |\n",
    "| 0   | 1   | 0   |\n",
    "| 1   | 0   | 0   |\n",
    "| 1   | 1   | 1   |\n",
    "\n",
    "From the truth table, we can see that the perceptron should output `1` only when both `x1` and `x2` are `1`. This means we need to set the weights and bias as follows:\n",
    "\n",
    "- `w1 = 10`\n",
    "- `w2 = 10`\n",
    "- `b = -15`\n",
    "\n",
    "With these values, the perceptron will compute the following:\n",
    "\n",
    "```text\n",
    "For (0, 0): z = 10*0 + 10*0 - 15 = -15 (output 0) i.e. 0.0000003 near to 0\n",
    "For (0, 1): z = 10*0 + 10*1 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 0): z = 10*1 + 10*0 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 1): z = 10*1 + 10*1 - 15 = 5 (output 1) i.e. 0.9933071 near to 1\n",
    "```\n",
    "\n",
    "As we can see, the perceptron correctly mimics the behavior of the AND gate.\n",
    "\n",
    "<hr>\n",
    "\n",
    "To conclude,\n",
    "\n",
    "we can see that the non-linear activation function (sigmoid) is able to generalize the `AND` gate with a `Single Perceptron`. Here, the weighted sum is the `Perceptron` output before applying the sigmoid function.\n",
    "\n",
    "### **The Equation of Boundary Line That Separates the Classes**\n",
    "\n",
    "The decision boundary for the AND gate can be represented by the equation:\n",
    "\n",
    "```bash\n",
    "z = 10*x1 + 10*x2 - 15\n",
    "\n",
    "and\n",
    "\n",
    "10*x1 + 10*x2 - 15 = 0 // The Sigmoid Function outputs 0.5 when z = 0\n",
    "\n",
    "So,\n",
    "\n",
    "x1 + x2 = 1.5\n",
    "\n",
    "or\n",
    "\n",
    "x2 = 1.5 - x1 i.e. y = mx + c\n",
    "```\n",
    "\n",
    "Because this equation defines a line in the 2D space (x1, x2) that separates the two classes (0 and 1).\n",
    "\n",
    "**Image**\n",
    "\n",
    "<img src='./Notes_Images/boundary_line.png'>\n",
    "\n",
    "### **Note**\n",
    "\n",
    "We just witnessed how a `Simple Single Perceptron` can model the behavior of an `AND` gate using a non-linear activation function.\n",
    "\n",
    "Now, imagine what `1000s` or `even millions` of these simple perceptrons can achieve when combined in a multi-layer architecture.\n",
    "\n",
    "Also, note that here we witnessed that for the `Non-Linear Activation Function` to give correct output, the combination of `Weights` and `Bias` should be carefully chosen.\n",
    "\n",
    "Therefore, the design of neural networks involves not just the architecture (how many layers, how many neurons per layer) but also the careful tuning of these parameters to achieve the desired performance.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The generalization rule for the `AND` becomes:\n",
    "\n",
    "The weights `w1` and `w2` should be positive and the bias `b` should be negative. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "But,\n",
    "\n",
    "The `Bias` should be a negative number that is bigger than the weighted sum of the inputs when they are both `1`. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "**Would `Sigmoid` be able to Generalize well, if the Weights and Bias are not carefully chosen?**\n",
    "\n",
    "No, `Sigmoid` would not be able to generalize well if the weights and bias are not carefully chosen. This is because the `Sigmoid` function is sensitive to the input values, and if the weights and bias do not create a suitable decision boundary, the output may not correctly represent the underlying data distribution.\n",
    "\n",
    "If the `Weights` i.e. `{10,10}` and `Bias` i.e. `{-5}` then the output of the `sigmoid` would be as below:\n",
    "\n",
    "```bash\n",
    "\n",
    "Gate: AND\n",
    "0 AND 0 = 0.00669285 i.e. 0 which is correct\n",
    "0 AND 1 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 0 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 1 = 1 i.e. 1 which is correct\n",
    "\n",
    "```\n",
    "\n",
    "Therefore, for the `Non-Linear` Activation function to work effectively, the weights and bias must be chosen carefully to create a suitable decision boundary.\n",
    "\n",
    "For that, Gradient Descent is often used to optimize the weights and bias during the training process.\n",
    "\n",
    "### **Follow Up Questions**\n",
    "\n",
    "**What if our inputs are not binary (0 or 1) but continuous values? How would that affect the design of the perceptron?**\n",
    "\n",
    "**What if we want to implement a different logical operation, such as OR or XOR? How would the design of the perceptron change in those cases?**\n",
    "\n",
    "**What if we want to implement a multi-class classification problem? How would the design of the perceptron change in that case?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a4624",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42e5c3",
   "metadata": {},
   "source": [
    "## **Our Perceptron as an AND Gate**\n",
    "\n",
    "Now let's try to implement our Perceptron as an AND gate. The AND gate outputs 1 only if both inputs are 1, otherwise, it outputs 0.\n",
    "\n",
    "```C++\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "using namespace std;\n",
    "\n",
    "int main()\n",
    "{\n",
    "  Perceptron p(2); // Object with 2 inputs on the Stack, No need to delete\n",
    "\n",
    "  p.set_weights({10, 10, -15}); // +1 Bias\n",
    "}\n",
    "\n",
    "cout << \"Gate: AND\" << endl;\n",
    "\n",
    "cout << \"0 AND 0 = \" << p.run({0,0}) << endl;\n",
    "cout << \"0 AND 1 = \" << p.run({0,1}) << endl;\n",
    "cout << \"1 AND 0 = \" << p.run({1,0}) << endl;\n",
    "cout << \"1 AND 1 = \" << p.run({1,1}) << endl;\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: AND\n",
    "// 0 AND 0 = 3.05902e-07\n",
    "// 0 AND 1 = 0.00669285\n",
    "// 1 AND 0 = 0.00669285\n",
    "// 1 AND 1 = 0.993307\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60017613",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89035573",
   "metadata": {},
   "source": [
    "## **OR Gate**\n",
    "\n",
    "<img src='./Notes_Images/or_gate.png'>\n",
    "\n",
    "The OR gate outputs 1 if at least one of the inputs is 1, otherwise, it outputs 0.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The weights should be `{15,15}` and the bias should be `-10`.\n",
    "\n",
    "The linear combination for the OR gate can be expressed as:\n",
    "\n",
    "```bash\n",
    "15x + 15y - 10 = 0\n",
    "\n",
    "then\n",
    "\n",
    "y = -x + 2/3\n",
    "\n",
    "```\n",
    "\n",
    "Below are the outputs of Sigmoid function for the OR gate:\n",
    "\n",
    "```C++\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: OR\n",
    "// 0 AND 0 = 4.53979e-05\n",
    "// 0 AND 1 = 0.993307\n",
    "// 1 AND 0 = 0.993307\n",
    "// 1 AND 1 = 1\n",
    "\n",
    "```\n",
    "\n",
    "**OR Gate Boundary Line Equation**\n",
    "\n",
    "<img src='./Notes_Images/or_gate_boundary.png'>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
