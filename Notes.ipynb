{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd15eedf",
   "metadata": {},
   "source": [
    "## **Resources**\n",
    "\n",
    "### **Training Neural Network with C++**\n",
    "\n",
    "[Linkedin_Learning](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/the-many-applications-of-machine-learning?autoSkip=true&resume=false&u=42288921)\n",
    "\n",
    "### **Understanding Neural Network in Depth**\n",
    "\n",
    "[Essential_Idea_Of_Neural_Network](https://www.youtube.com/watch?v=CqOfi41LfDw)\n",
    "\n",
    "[How_CNN_Works_in_Depth](https://www.youtube.com/watch?v=JB8T_zN7ZC0)\n",
    "\n",
    "### **The Mathematics Behind Neural Network**\n",
    "\n",
    "[Maths_Behind_Neural_Network](https://www.youtube.com/watch?v=Ixl3nykKG9M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71513c55",
   "metadata": {},
   "source": [
    "# **Tomorrow**\n",
    "\n",
    "[OR_Gate](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/solution-logic-gates-with-perceptrons?resume=false&u=42288921)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61e01b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162004",
   "metadata": {},
   "source": [
    "## **Neural Network Implementation Note**\n",
    "\n",
    "- All values must be real numbers, not integers. We will use double point precision (e.g., 0.1, 0.2).\n",
    "\n",
    "- The weights and inputs may be implemented as `1-D` vectors. We will use the `std::vector<double>` type from the C++ Standard Library i.e. `vec(w)` and `vec(x)`.\n",
    "\n",
    "- This way, the sum may be calculated in one operation: `z = vec(w) * vec(x)`.\n",
    "\n",
    "- We will feed the weighted sum to the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85211873",
   "metadata": {},
   "source": [
    "## **Files and Their Meaning**\n",
    "\n",
    "**`.h` files**: These are header files in C++ that typically contain function declarations, class definitions, and macros. They are included in `.cpp` files to provide the necessary declarations for the functions and classes used in the implementation.\n",
    "\n",
    "**`.cpp` files**: These are source files in C++ that contain the actual implementation of the functions and classes declared in the corresponding header files. They are compiled to create the final executable program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74332d50",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33287",
   "metadata": {},
   "source": [
    "## **Built in Functions That We Will Use**\n",
    "\n",
    "`std::vector`: A dynamic array that can resize itself automatically when elements are added or removed.\n",
    "\n",
    "Syntax: `std::vector<Type> vec;`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::inner_product`: Computes the inner product of two ranges.\n",
    "\n",
    "Syntax: `std::inner_product(first1, last1, first2, init);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::generate`: Fills a range with values generated by a function.\n",
    "\n",
    "Syntax: `std::generate(first, last, generator);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::push_back`: Adds an element to the end of a vector.\n",
    "\n",
    "Syntax: `vec.push_back(value);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::resize`: Changes the size of a vector.\n",
    "\n",
    "Syntax: `vec.resize(new_size);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::exp`: Computes the exponential function.\n",
    "\n",
    "Syntax: `std::exp(x);`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5671d9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2128f",
   "metadata": {},
   "source": [
    "## **Neural Network into Action**\n",
    "\n",
    "We will write all the declarations in the header files and all the implementations in the source files. This will help us keep our code organized and modular.\n",
    "\n",
    "Our first task it to implement basic `Multi Layer Perceptron` class in C++.\n",
    "\n",
    "For that we are creating `MLP.h` and `MLP.cpp` files.\n",
    "\n",
    "### **`MLP.h`**\n",
    "\n",
    "```C++\n",
    "\n",
    "// Perceptron class\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  // Run the perceptron\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  // Set Custom Weights if needed\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  // Sigmoid Activation Function\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "```\n",
    "\n",
    "Here, `size_t` is used to represent the number of inputs to the perceptron, ensuring that the value is always non-negative. It is an `unsigned integer` type which store `8 Bytes` in 64 Bit System and `4 Bytes` in 32 Bit System.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we'll implement the `Perceptron` class in the `MLP.cpp`.\n",
    "\n",
    "### **`MLP.cpp`**\n",
    "\n",
    "Here, we will write the implementation of the `Perceptron` class.\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "```\n",
    "\n",
    "**Below is the Step wise Step Explanation for Each Implementation.**\n",
    "\n",
    "`weights.resize(inputs + 1);`\n",
    "\n",
    "This line resizes the weights vector to hold the specified number of inputs plus one `additional` element for the bias. This ensures that the weights vector has the correct size to accommodate all input weights and the bias term.\n",
    "\n",
    "`generate(weights.begin(), weights.end(), frand);`\n",
    "\n",
    "This line fills the weights vector with random values generated by the `frand` function. The `generate` function takes a range (from the beginning to the end of the weights vector) and applies the `frand` function to each element in that range, effectively initializing the weights to small random values.\n",
    "\n",
    "`x.push_back(bias);`\n",
    "\n",
    "This line adds the bias term to the end of the input vector `x`. This is necessary because the bias is treated as an additional input to the perceptron, and it needs to be included in the weighted sum calculation.\n",
    "\n",
    "`inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);`\n",
    "\n",
    "This line computes the weighted sum of the inputs by taking the inner product of the input vector `x` (which now includes the bias) and the weights vector. The `inner_product` function multiplies each element of the input vector by the corresponding element of the weights vector and sums the results. The last argument `(double)0.0` specifies the initial value for the sum.\n",
    "\n",
    "`return sigmoid(sum);`\n",
    "\n",
    "This line passes the computed weighted sum into the sigmoid function and returns the result. The sigmoid function applies the logistic activation function to the weighted sum, squashing the output to a range between 0 and 1. This is a crucial step in the perceptron's operation, as it determines the final output of the neuron.\n",
    "\n",
    "`weights = w_init;`\n",
    "\n",
    "This line sets the weights of the perceptron to the provided initialization vector `w_init`. This allows the user to specify custom weights for the perceptron, which can be useful for tasks like transfer learning or fine-tuning a pre-trained model.\n",
    "\n",
    "`return 1.0 / (1.0 + exp(-x));`\n",
    "\n",
    "Calculates the sigmoid activation value for the given input `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e343f1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7649416",
   "metadata": {},
   "source": [
    "## **AND Gate**\n",
    "\n",
    "Both the inputs need to be `True` for `True` output.\n",
    "\n",
    "Now how do we create a Perceptron that can classify inputs like an AND gate?\n",
    "\n",
    "Let's visualize the inputs and outputs of the `AND` gate in a Graph:\n",
    "\n",
    "<img src='./Notes_Images/and_gate.png'>\n",
    "\n",
    "Now, to successfully classify we need to draw a line that separates the two classes (0 and 1). This line is called the decision boundary.\n",
    "\n",
    "<img src='./Notes_Images/boundary.png'>\n",
    "\n",
    "**The Line that is To be Drawn is of Sigmoid Function**\n",
    "\n",
    "<img src='./Notes_Images/sigmoid.png'>\n",
    "\n",
    "In this image, the boundary is the line where sigmoid is `0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Before Moving Forward`,\n",
    "\n",
    "Let's try to implement a function that exactly mimics as `AND` gate, but the function should be linear i.e. `f(x1, x2) = w1*x1 + w2*x2 + b`\n",
    "\n",
    "**Is that Possible?**\n",
    "\n",
    "<img src='./Notes_Images/linear_and.png'>\n",
    "\n",
    "This proof shows that it is not possible to create a linear function that mimics the behavior of an `AND` gate.\n",
    "\n",
    "The only solution is that the function should be non-linear, which means the function can be `exponential`, `quadratic`, or any other non-linear form.\n",
    "\n",
    "<img src='./Notes_Images/non_linear_and.png'>\n",
    "\n",
    "**TL;DR: AND is `linearly separable` (a perceptron can classify it), but it is not a linear function of the `inputs`.**\n",
    "\n",
    "### **A Perceptron as an AND Gate**\n",
    "\n",
    "Let's say there are two inputs `x1` and `x2`. The perceptron will compute a weighted sum of the inputs and pass it through a step function to produce the output.\n",
    "\n",
    "The weighted sum can be represented as:\n",
    "\n",
    "```\n",
    "z = w1*x1 + w2*x2 + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `w1` and `w2` are the weights for the inputs\n",
    "- `b` is the bias term\n",
    "\n",
    "<hr>\n",
    "\n",
    "The earlier problem was that we were not able to find a linear function that could separate the two classes (0 and 1).\n",
    "\n",
    "But if we pass the output of the `linear function` i.e. `z = w1*x1 + w2*x2 + b` through a `non-linear activation` function i.e. `sigmoid`, we can achieve the desired results.\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "```\n",
    "σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where `e` is the base of the natural logarithm.\n",
    "\n",
    "When the value of `z` is `0`, the sigmoid function outputs `0.5`.\n",
    "\n",
    "When `z` is positive, the sigmoid function outputs a value between `0.5` and `1`. When `z` is negative, the sigmoid function outputs a value between `0` and `0.5`.\n",
    "\n",
    "For positive value, the output converges to `1` as `z` increases. For negative value, the output converges to `0` as `z` decreases.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The step function will output `1` if `z` is greater than or equal to `0`, and `0` otherwise.\n",
    "\n",
    "To implement the AND gate, we need to find appropriate values for `w1`, `w2`, and `b` such that the perceptron produces the correct output for all possible combinations of inputs.\n",
    "\n",
    "The truth table for the AND gate is as follows:\n",
    "\n",
    "| x1  | x2  | AND |\n",
    "| --- | --- | --- |\n",
    "| 0   | 0   | 0   |\n",
    "| 0   | 1   | 0   |\n",
    "| 1   | 0   | 0   |\n",
    "| 1   | 1   | 1   |\n",
    "\n",
    "From the truth table, we can see that the perceptron should output `1` only when both `x1` and `x2` are `1`. This means we need to set the weights and bias as follows:\n",
    "\n",
    "- `w1 = 10`\n",
    "- `w2 = 10`\n",
    "- `b = -15`\n",
    "\n",
    "With these values, the perceptron will compute the following:\n",
    "\n",
    "```text\n",
    "For (0, 0): z = 10*0 + 10*0 - 15 = -15 (output 0) i.e. 0.0000003 near to 0\n",
    "For (0, 1): z = 10*0 + 10*1 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 0): z = 10*1 + 10*0 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 1): z = 10*1 + 10*1 - 15 = 5 (output 1) i.e. 0.9933071 near to 1\n",
    "```\n",
    "\n",
    "As we can see, the perceptron correctly mimics the behavior of the AND gate.\n",
    "\n",
    "<hr>\n",
    "\n",
    "To conclude,\n",
    "\n",
    "we can see that the non-linear activation function (sigmoid) is able to generalize the `AND` gate with a `Single Perceptron`. Here, the weighted sum is the `Perceptron` output before applying the sigmoid function.\n",
    "\n",
    "### **The Equation of Boundary Line That Separates the Classes**\n",
    "\n",
    "The decision boundary for the AND gate can be represented by the equation:\n",
    "\n",
    "```bash\n",
    "z = 10*x1 + 10*x2 - 15\n",
    "\n",
    "and\n",
    "\n",
    "10*x1 + 10*x2 - 15 = 0 // The Sigmoid Function outputs 0.5 when z = 0\n",
    "\n",
    "So,\n",
    "\n",
    "x1 + x2 = 1.5\n",
    "\n",
    "or\n",
    "\n",
    "x2 = 1.5 - x1 i.e. y = mx + c\n",
    "```\n",
    "\n",
    "Because this equation defines a line in the 2D space (x1, x2) that separates the two classes (0 and 1).\n",
    "\n",
    "**Image**\n",
    "\n",
    "<img src='./Notes_Images/boundary_line.png'>\n",
    "\n",
    "### **Note**\n",
    "\n",
    "We just witnessed how a `Simple Single Perceptron` can model the behavior of an `AND` gate using a non-linear activation function.\n",
    "\n",
    "Now, imagine what `1000s` or `even millions` of these simple perceptrons can achieve when combined in a multi-layer architecture.\n",
    "\n",
    "Also, note that here we witnessed that for the `Non-Linear Activation Function` to give correct output, the combination of `Weights` and `Bias` should be carefully chosen.\n",
    "\n",
    "Therefore, the design of neural networks involves not just the architecture (how many layers, how many neurons per layer) but also the careful tuning of these parameters to achieve the desired performance.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The generalization rule for the `AND` becomes:\n",
    "\n",
    "The weights `w1` and `w2` should be positive and the bias `b` should be negative. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "But,\n",
    "\n",
    "The `Bias` should be a negative number that is bigger than the weighted sum of the inputs when they are both `1`. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "**Would `Sigmoid` be able to Generalize well, if the Weights and Bias are not carefully chosen?**\n",
    "\n",
    "No, `Sigmoid` would not be able to generalize well if the weights and bias are not carefully chosen. This is because the `Sigmoid` function is sensitive to the input values, and if the weights and bias do not create a suitable decision boundary, the output may not correctly represent the underlying data distribution.\n",
    "\n",
    "If the `Weights` i.e. `{10,10}` and `Bias` i.e. `{-5}` then the output of the `sigmoid` would be as below:\n",
    "\n",
    "```bash\n",
    "\n",
    "Gate: AND\n",
    "0 AND 0 = 0.00669285 i.e. 0 which is correct\n",
    "0 AND 1 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 0 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 1 = 1 i.e. 1 which is correct\n",
    "\n",
    "```\n",
    "\n",
    "Therefore, for the `Non-Linear` Activation function to work effectively, the weights and bias must be chosen carefully to create a suitable decision boundary.\n",
    "\n",
    "For that, Gradient Descent is often used to optimize the weights and bias during the training process.\n",
    "\n",
    "### **Follow Up Questions**\n",
    "\n",
    "**What if our inputs are not binary (0 or 1) but continuous values? How would that affect the design of the perceptron?**\n",
    "\n",
    "**What if we want to implement a different logical operation, such as OR or XOR? How would the design of the perceptron change in those cases?**\n",
    "\n",
    "**What if we want to implement a multi-class classification problem? How would the design of the perceptron change in that case?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a4624",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42e5c3",
   "metadata": {},
   "source": [
    "## **Our Perceptron as an AND Gate**\n",
    "\n",
    "Now let's try to implement our Perceptron as an AND gate. The AND gate outputs 1 only if both inputs are 1, otherwise, it outputs 0.\n",
    "\n",
    "```C++\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "using namespace std;\n",
    "\n",
    "int main()\n",
    "{\n",
    "  Perceptron p(2); // Object with 2 inputs on the Stack, No need to delete\n",
    "\n",
    "  p.set_weights({10, 10, -15}); // +1 Bias\n",
    "}\n",
    "\n",
    "cout << \"Gate: AND\" << endl;\n",
    "\n",
    "cout << \"0 AND 0 = \" << p.run({0,0}) << endl;\n",
    "cout << \"0 AND 1 = \" << p.run({0,1}) << endl;\n",
    "cout << \"1 AND 0 = \" << p.run({1,0}) << endl;\n",
    "cout << \"1 AND 1 = \" << p.run({1,1}) << endl;\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: AND\n",
    "// 0 AND 0 = 3.05902e-07\n",
    "// 0 AND 1 = 0.00669285\n",
    "// 1 AND 0 = 0.00669285\n",
    "// 1 AND 1 = 0.993307\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60017613",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89035573",
   "metadata": {},
   "source": [
    "## **OR Gate**\n",
    "\n",
    "<img src='./Notes_Images/or_gate.png'>\n",
    "\n",
    "The OR gate outputs 1 if at least one of the inputs is 1, otherwise, it outputs 0.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The weights should be `{15,15}` and the bias should be `-10`.\n",
    "\n",
    "The linear combination for the OR gate can be expressed as:\n",
    "\n",
    "```bash\n",
    "15x + 15y - 10 = 0\n",
    "\n",
    "then\n",
    "\n",
    "y = -x + 2/3\n",
    "\n",
    "```\n",
    "\n",
    "Below are the outputs of Sigmoid function for the OR gate:\n",
    "\n",
    "```C++\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: OR\n",
    "// 0 AND 0 = 4.53979e-05\n",
    "// 0 AND 1 = 0.993307\n",
    "// 1 AND 0 = 0.993307\n",
    "// 1 AND 1 = 1\n",
    "\n",
    "```\n",
    "\n",
    "**OR Gate Boundary Line Equation**\n",
    "\n",
    "<img src='./Notes_Images/or_gate_boundary.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab66ef",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807be646",
   "metadata": {},
   "source": [
    "## **Linear Separability**\n",
    "\n",
    "`Linear separability` is a property of a dataset that allows it to be separated into different classes using a `linear boundary`. In the context of neural networks, this means that a single layer perceptron can be used to classify the data points.\n",
    "\n",
    "For example, the `AND` gate is linearly separable because we can draw a `straight line` (or `hyperplane` in higher dimensions) that separates the positive examples (1s) from the negative examples (0s).\n",
    "\n",
    "Similarly, the `OR` gate is also linearly separable for the same reason.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- Both the `Straight Line` i.e. `y = mx + b` and the `Hyperplane` i.e. `W*x + b = 0` can be used to separate linearly separable data.\n",
    "\n",
    "- For 2 dimensional data i.e. 2 input features we need the equation of line, for 3 dimensional data, we need the equation of plane i.e. `Ax + By + Cz + D = 0` and for data whose dimension is greater than 3, we need the equation of hyperplane i.e. `W*x + b = 0`, where `W` is the weight vector and `b` is the bias.\n",
    "\n",
    "<hr>\n",
    "\n",
    "On the other hand, the `XOR` gate is not linearly separable because there is no single straight line that can separate the positive examples from the negative examples.\n",
    "\n",
    "Below is the Graphical representation of the `XOR` gate:\n",
    "\n",
    "<img src='./Notes_Images/xor_gate.png'>\n",
    "\n",
    "Here, we cannot separate the positive and negative examples with a single straight line. We would need two lines to separate the classes.\n",
    "\n",
    "If we use an `OR` gate only, it will get all but one of the `XOR` gate inputs correct. The `OR` gate will output `1` for both `(0, 1)` and `(1, 0)` inputs, which is incorrect for the `XOR` operation.\n",
    "\n",
    "<img src='./Notes_Images/or_for_xor.png'>\n",
    "\n",
    "If we use `NAND` gate, it will give one incorrect output for the `(0, 0)` input, which is the only case where the `XOR` gate outputs `1`.\n",
    "\n",
    "<img src='./Notes_Images/nand_for_xor.png'>\n",
    "\n",
    "**But**\n",
    "\n",
    "If we combine the outputs of the `NAND` gate and the `OR` gate, we can create a circuit that correctly implements the `XOR` function.\n",
    "\n",
    "<img src='./Notes_Images/nand_or_xor.png'>\n",
    "\n",
    "### **Creating XOR with NAND, AND, and OR Gates**\n",
    "\n",
    "To create an `XOR` gate using `NAND`, `AND`, and `OR` gates, we can use the following configuration:\n",
    "\n",
    "1. **Inputs**: A and B\n",
    "\n",
    "2. **NAND Gate**: Connect A and B to a `NAND` gate. This will give us the output `NAND(A, B)`.\n",
    "\n",
    "3. **OR Gate**: Connect A and B to an `OR` gate. This will give us the output `OR(A, B)`.\n",
    "\n",
    "4. **AND Gate**: Connect the outputs of the `NAND` gate and the `OR` gate to an `AND` gate. This will give us the final output `XOR(A, B)`.\n",
    "\n",
    "The logical expression for the `XOR` gate can be represented as:\n",
    "\n",
    "```bash\n",
    "XOR(A, B) = AND(NAND(A, B), OR(A, B))\n",
    "```\n",
    "\n",
    "This configuration allows us to implement the `XOR` function using only `NAND`, `AND`, and `OR` gates.\n",
    "\n",
    "**XOR Diagram**\n",
    "\n",
    "<img src='./Notes_Images/xor_diagram.png'>\n",
    "\n",
    "### **Neural Network for XOR Gate**\n",
    "\n",
    "We know a single perceptron can solve a linear separable problem, but the `XOR` function is not linearly separable. Therefore, we need a neural network with `3 perceptrons` i.e. `2 in the hidden layer and 1 in the output layer`.\n",
    "\n",
    "Also, we know that a single perceptron can represent all the three basic logic gates: `AND`, `OR`, and `NAND` each having different `weight` and `bias` configurations.\n",
    "\n",
    "**Linear Equations for Basic Logic Gates**\n",
    "\n",
    "`OR Gate` : `y = -x + 0.5` with Weights `{15,15}` and Bias `{-10}`\n",
    "\n",
    "`NAND Gate` : `y = x + 1.5` with Weights `{-10,-10}` and Bias `{15}`\n",
    "\n",
    "Then we plug the output of `OR Gate` and `NAND Gate` as an Input for the `AND Gate`\n",
    "\n",
    "`AND Gate` : `y = -x + 1.5` with Weights `{10.10}` and Bias `{-15}`\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11968e3",
   "metadata": {},
   "source": [
    "## **Multi Layer Perceptron**\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of neural network that consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. MLPs are capable of learning complex patterns in data and can be used for a variety of tasks, including classification and regression.\n",
    "\n",
    "**Image of MLP for XOR Gate**\n",
    "\n",
    "<img src='./Notes_Images/mlp.png'/>\n",
    "\n",
    "### **Architecture of MLP for XOR Gate**\n",
    "\n",
    "1. **Input Layer**: The input layer consists of two inputs, each representing one of the input features (X1 and X2) of the XOR gate.\n",
    "\n",
    "2. **Hidden Layer**: The hidden layer contains two neurons. One of the neurons represents the `NAND Gate` operation, while the other represents the `OR` operation.\n",
    "\n",
    "3. **Output Layer**: The output layer consists of a single neuron that produces the final output of the network. This neuron receives inputs from both hidden layer neurons, applies a weighted sum and a non-linear activation function, and produces the final output (Y) of the `XOR` gate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bf1a7",
   "metadata": {},
   "source": [
    "## **XOR Gate Implementation**\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the `mlp.h` file previously we had written declaration for the `Single Perceptron` class. Now, we will extend this class to create a `MultiLayerPerceptron` class that can handle the XOR problem.\n",
    "\n",
    "### **`mlp.h`**\n",
    "\n",
    "```C++\n",
    "#pragma once\n",
    "\n",
    "#include <algorithm>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <numeric>\n",
    "#include <cmath>\n",
    "#include <time.h>\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  // Run the Perceptron\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  // Set the Customize Weights if Needed\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  // Sigmoid Activation Function\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "\n",
    "class MultiLayerPerceptron\n",
    "{\n",
    "public:\n",
    "  // Constructor for initilizing layers\n",
    "  MultiLayerPerceptron(std::vector<size_t> layers, double bias = 1.0, double eta = 0.5);\n",
    "\n",
    "  // Set custom weights, w_init for weights of 3 perceptron\n",
    "  void set_weights(std::vector<std::vector<std::vector<double>>> w_init);\n",
    "\n",
    "  // Display the weights\n",
    "  void print_weights();\n",
    "\n",
    "  // Run the MLP\n",
    "  std::vector<double> run(std::vector<double> x);\n",
    "\n",
    "  // For Backpropagation\n",
    "  double bp(std::vector<double> x, std::vector<double> y);\n",
    "\n",
    "  // Attributes\n",
    "\n",
    "  std::vector<size_t> layers; // Unsigned Integers, Number of Neurons Per Layer, 0 for Input, 2 for Hidden, 1 for Output\n",
    "\n",
    "  double bias; // Bias\n",
    "  double eta;  // Learning Rate\n",
    "\n",
    "  std::vector<std::vector<Perceptron>> network; // Neural Network\n",
    "  std::vector<std::vector<double>> values;      // Hodl the Output Valuse of the Network\n",
    "  std::vector<std::vector<double>> d;           // Error Terms for the Neurons\n",
    "};\n",
    "```\n",
    "\n",
    "In the above code,\n",
    "\n",
    "The `MultiLayerPerceptron` class is designed to handle the XOR problem by utilizing multiple layers of neurons.\n",
    "\n",
    "**`MultiLayerPerceptron(std::vector<size_t> layers, double bias = 1.0, double eta = 0.5)`** : This constructor initializes the MLP with the specified layer structure, bias, and learning rate. It creates the necessary layers and populates the network with `Perceptron` objects.\n",
    "\n",
    "**`run(std::vector<double> x)`** : This method takes an input vector `x` and passes it through the network, returning the output of the MLP.\n",
    "\n",
    "**`bp(std::vector<double> x, std::vector<double> y)`** : This method performs backpropagation to update the weights of the network based on the error between the predicted output and the true output `y`.\n",
    "\n",
    "**`std::vector<std::vector<Perceptron>> network;`** : This attribute holds the layers of the MLP, where each layer is a vector of `Perceptron` objects.\n",
    "\n",
    "**`std::vector<std::vector<double>> values;`** : This attribute stores the output values of each neuron in the network for a given input.\n",
    "\n",
    "**`std::vector<std::vector<double>> d;`** : This attribute holds the error terms for the neurons, which are used during backpropagation to update the weights.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **`mlp.cpp`**\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "/*\n",
    "Single Layer Perceptron Implementation\n",
    "*/\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "\n",
    "/*\n",
    "Multi Layer Perceptron Implementation\n",
    "*/\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "MultiLayerPerceptron::MultiLayerPerceptron(std::vector<size_t> layers, double bias, double eta) : layers(layers), bias(bias), eta(eta)\n",
    "{\n",
    "  // Create Neurons Layer By Layer\n",
    "\n",
    "  for (size_t i = 0; i < layers.size(); i++)\n",
    "  {\n",
    "    // Add Vector of Values Filled with Zeros\n",
    "    values.push_back(vector<double>(layers[i], 0.0)); // Output of Each Neuron Value set to Zero based on the number of Neurons in Each layer\n",
    "\n",
    "    // Add Vector of Neurons\n",
    "    network.push_back(vector<Perceptron>());\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Here, in the above code for `MLP`,\n",
    "\n",
    "**`MultiLayerPerceptron::MultiLayerPerceptron(std::vector<size_t> layers, double bias, double eta)`** : This constructor initializes the MLP with the specified layer structure, bias, and learning rate. It creates the necessary layers and populates the network with `Perceptron` objects.\n",
    "\n",
    "```C++\n",
    "\n",
    "// Create Neurons Layer By Layer\n",
    "\n",
    "for (size_t i = 0; i < layers.size(); i++)\n",
    "{\n",
    "  // Add Vector of Values Filled with Zeros\n",
    "  values.push_back(vector<double>(layers[i], 0.0));\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The above code creates the necessary layers for the MLP by adding vectors of zeros for each layer. This sets up the structure for the neuron outputs, which will be filled during the forward pass of the network.\n",
    "\n",
    "`vector<double>(layers[i], 0.0)` : Uses the `std::vector` constructor to create a vector of the specified size (`layers[i]`) initialized with zeros (`0.0`).\n",
    "\n",
    "```C++\n",
    "// Add Vector of Neurons\n",
    "network.push_back(vector<Perceptron>()); // Perceptron Constructor, Empty for Now\n",
    "```\n",
    "\n",
    "The above code adds a new vector of `Perceptron` objects for each layer in the network. This sets up the structure for the neurons in each layer, which will be initialized with random weights during the training process.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
