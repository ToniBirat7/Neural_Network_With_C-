{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd15eedf",
   "metadata": {},
   "source": [
    "## **Resources**\n",
    "\n",
    "### **Training Neural Network with C++**\n",
    "\n",
    "[Linkedin_Learning](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/the-many-applications-of-machine-learning?autoSkip=true&resume=false&u=42288921)\n",
    "\n",
    "### **Understanding Neural Network in Depth**\n",
    "\n",
    "[Essential_Idea_Of_Neural_Network](https://www.youtube.com/watch?v=CqOfi41LfDw)\n",
    "\n",
    "[How_CNN_Works_in_Depth](https://www.youtube.com/watch?v=JB8T_zN7ZC0)\n",
    "\n",
    "### **The Mathematics Behind Neural Network**\n",
    "\n",
    "[Maths_Behind_Neural_Network](https://www.youtube.com/watch?v=Ixl3nykKG9M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71513c55",
   "metadata": {},
   "source": [
    "# **Tomorrow**\n",
    "\n",
    "[Cont_Run](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/solution-finish-the-multilayer-perceptron-class?resume=false&u=42288921)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61e01b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162004",
   "metadata": {},
   "source": [
    "## **Neural Network Implementation Note**\n",
    "\n",
    "- All values must be real numbers, not integers. We will use double point precision (e.g., 0.1, 0.2).\n",
    "\n",
    "- The weights and inputs may be implemented as `1-D` vectors. We will use the `std::vector<double>` type from the C++ Standard Library i.e. `vec(w)` and `vec(x)`.\n",
    "\n",
    "- This way, the sum may be calculated in one operation: `z = vec(w) * vec(x)`.\n",
    "\n",
    "- We will feed the weighted sum to the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85211873",
   "metadata": {},
   "source": [
    "## **Files and Their Meaning**\n",
    "\n",
    "**`.h` files**: These are header files in C++ that typically contain function declarations, class definitions, and macros. They are included in `.cpp` files to provide the necessary declarations for the functions and classes used in the implementation.\n",
    "\n",
    "**`.cpp` files**: These are source files in C++ that contain the actual implementation of the functions and classes declared in the corresponding header files. They are compiled to create the final executable program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74332d50",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33287",
   "metadata": {},
   "source": [
    "## **Built in Functions That We Will Use**\n",
    "\n",
    "`std::vector`: A dynamic array that can resize itself automatically when elements are added or removed.\n",
    "\n",
    "Syntax: `std::vector<Type> vec;`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::inner_product`: Computes the inner product of two ranges.\n",
    "\n",
    "Syntax: `std::inner_product(first1, last1, first2, init);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::generate`: Fills a range with values generated by a function.\n",
    "\n",
    "Syntax: `std::generate(first, last, generator);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::push_back`: Adds an element to the end of a vector.\n",
    "\n",
    "Syntax: `vec.push_back(value);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::resize`: Changes the size of a vector.\n",
    "\n",
    "Syntax: `vec.resize(new_size);`\n",
    "\n",
    "<hr>\n",
    "\n",
    "`std::exp`: Computes the exponential function.\n",
    "\n",
    "Syntax: `std::exp(x);`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5671d9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2128f",
   "metadata": {},
   "source": [
    "## **Neural Network into Action**\n",
    "\n",
    "We will write all the declarations in the header files and all the implementations in the source files. This will help us keep our code organized and modular.\n",
    "\n",
    "Our first task it to implement basic `Multi Layer Perceptron` class in C++.\n",
    "\n",
    "For that we are creating `MLP.h` and `MLP.cpp` files.\n",
    "\n",
    "### **`MLP.h`**\n",
    "\n",
    "```C++\n",
    "\n",
    "// Perceptron class\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  // Run the perceptron\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  // Set Custom Weights if needed\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  // Sigmoid Activation Function\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "```\n",
    "\n",
    "Here, `size_t` is used to represent the number of inputs to the perceptron, ensuring that the value is always non-negative. It is an `unsigned integer` type which store `8 Bytes` in 64 Bit System and `4 Bytes` in 32 Bit System.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we'll implement the `Perceptron` class in the `MLP.cpp`.\n",
    "\n",
    "### **`MLP.cpp`**\n",
    "\n",
    "Here, we will write the implementation of the `Perceptron` class.\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "```\n",
    "\n",
    "**Below is the Step wise Step Explanation for Each Implementation.**\n",
    "\n",
    "`weights.resize(inputs + 1);`\n",
    "\n",
    "This line resizes the weights vector to hold the specified number of inputs plus one `additional` element for the bias. This ensures that the weights vector has the correct size to accommodate all input weights and the bias term.\n",
    "\n",
    "`generate(weights.begin(), weights.end(), frand);`\n",
    "\n",
    "This line fills the weights vector with random values generated by the `frand` function. The `generate` function takes a range (from the beginning to the end of the weights vector) and applies the `frand` function to each element in that range, effectively initializing the weights to small random values.\n",
    "\n",
    "`x.push_back(bias);`\n",
    "\n",
    "This line adds the bias term to the end of the input vector `x`. This is necessary because the bias is treated as an additional input to the perceptron, and it needs to be included in the weighted sum calculation.\n",
    "\n",
    "`inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);`\n",
    "\n",
    "This line computes the weighted sum of the inputs by taking the inner product of the input vector `x` (which now includes the bias) and the weights vector. The `inner_product` function multiplies each element of the input vector by the corresponding element of the weights vector and sums the results. The last argument `(double)0.0` specifies the initial value for the sum.\n",
    "\n",
    "`return sigmoid(sum);`\n",
    "\n",
    "This line passes the computed weighted sum into the sigmoid function and returns the result. The sigmoid function applies the logistic activation function to the weighted sum, squashing the output to a range between 0 and 1. This is a crucial step in the perceptron's operation, as it determines the final output of the neuron.\n",
    "\n",
    "`weights = w_init;`\n",
    "\n",
    "This line sets the weights of the perceptron to the provided initialization vector `w_init`. This allows the user to specify custom weights for the perceptron, which can be useful for tasks like transfer learning or fine-tuning a pre-trained model.\n",
    "\n",
    "`return 1.0 / (1.0 + exp(-x));`\n",
    "\n",
    "Calculates the sigmoid activation value for the given input `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e343f1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7649416",
   "metadata": {},
   "source": [
    "## **AND Gate**\n",
    "\n",
    "Both the inputs need to be `True` for `True` output.\n",
    "\n",
    "Now how do we create a Perceptron that can classify inputs like an AND gate?\n",
    "\n",
    "Let's visualize the inputs and outputs of the `AND` gate in a Graph:\n",
    "\n",
    "<img src='./Notes_Images/and_gate.png'>\n",
    "\n",
    "Now, to successfully classify we need to draw a line that separates the two classes (0 and 1). This line is called the decision boundary.\n",
    "\n",
    "<img src='./Notes_Images/boundary.png'>\n",
    "\n",
    "**The Line that is To be Drawn is of Sigmoid Function**\n",
    "\n",
    "<img src='./Notes_Images/sigmoid.png'>\n",
    "\n",
    "In this image, the boundary is the line where sigmoid is `0.5`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Before Moving Forward`,\n",
    "\n",
    "Let's try to implement a function that exactly mimics as `AND` gate, but the function should be linear i.e. `f(x1, x2) = w1*x1 + w2*x2 + b`\n",
    "\n",
    "**Is that Possible?**\n",
    "\n",
    "<img src='./Notes_Images/linear_and.png'>\n",
    "\n",
    "This proof shows that it is not possible to create a linear function that mimics the behavior of an `AND` gate.\n",
    "\n",
    "The only solution is that the function should be non-linear, which means the function can be `exponential`, `quadratic`, or any other non-linear form.\n",
    "\n",
    "<img src='./Notes_Images/non_linear_and.png'>\n",
    "\n",
    "**TL;DR: AND is `linearly separable` (a perceptron can classify it), but it is not a linear function of the `inputs`.**\n",
    "\n",
    "### **A Perceptron as an AND Gate**\n",
    "\n",
    "Let's say there are two inputs `x1` and `x2`. The perceptron will compute a weighted sum of the inputs and pass it through a step function to produce the output.\n",
    "\n",
    "The weighted sum can be represented as:\n",
    "\n",
    "```\n",
    "z = w1*x1 + w2*x2 + b\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `w1` and `w2` are the weights for the inputs\n",
    "- `b` is the bias term\n",
    "\n",
    "<hr>\n",
    "\n",
    "The earlier problem was that we were not able to find a linear function that could separate the two classes (0 and 1).\n",
    "\n",
    "But if we pass the output of the `linear function` i.e. `z = w1*x1 + w2*x2 + b` through a `non-linear activation` function i.e. `sigmoid`, we can achieve the desired results.\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "```\n",
    "σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where `e` is the base of the natural logarithm.\n",
    "\n",
    "When the value of `z` is `0`, the sigmoid function outputs `0.5`.\n",
    "\n",
    "When `z` is positive, the sigmoid function outputs a value between `0.5` and `1`. When `z` is negative, the sigmoid function outputs a value between `0` and `0.5`.\n",
    "\n",
    "For positive value, the output converges to `1` as `z` increases. For negative value, the output converges to `0` as `z` decreases.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The step function will output `1` if `z` is greater than or equal to `0`, and `0` otherwise.\n",
    "\n",
    "To implement the AND gate, we need to find appropriate values for `w1`, `w2`, and `b` such that the perceptron produces the correct output for all possible combinations of inputs.\n",
    "\n",
    "The truth table for the AND gate is as follows:\n",
    "\n",
    "| x1  | x2  | AND |\n",
    "| --- | --- | --- |\n",
    "| 0   | 0   | 0   |\n",
    "| 0   | 1   | 0   |\n",
    "| 1   | 0   | 0   |\n",
    "| 1   | 1   | 1   |\n",
    "\n",
    "From the truth table, we can see that the perceptron should output `1` only when both `x1` and `x2` are `1`. This means we need to set the weights and bias as follows:\n",
    "\n",
    "- `w1 = 10`\n",
    "- `w2 = 10`\n",
    "- `b = -15`\n",
    "\n",
    "With these values, the perceptron will compute the following:\n",
    "\n",
    "```text\n",
    "For (0, 0): z = 10*0 + 10*0 - 15 = -15 (output 0) i.e. 0.0000003 near to 0\n",
    "For (0, 1): z = 10*0 + 10*1 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 0): z = 10*1 + 10*0 - 15 = -5 (output 0) i.e. 0.0066929 near to 0\n",
    "For (1, 1): z = 10*1 + 10*1 - 15 = 5 (output 1) i.e. 0.9933071 near to 1\n",
    "```\n",
    "\n",
    "As we can see, the perceptron correctly mimics the behavior of the AND gate.\n",
    "\n",
    "<hr>\n",
    "\n",
    "To conclude,\n",
    "\n",
    "we can see that the non-linear activation function (sigmoid) is able to generalize the `AND` gate with a `Single Perceptron`. Here, the weighted sum is the `Perceptron` output before applying the sigmoid function.\n",
    "\n",
    "### **The Equation of Boundary Line That Separates the Classes**\n",
    "\n",
    "The decision boundary for the AND gate can be represented by the equation:\n",
    "\n",
    "```bash\n",
    "z = 10*x1 + 10*x2 - 15\n",
    "\n",
    "and\n",
    "\n",
    "10*x1 + 10*x2 - 15 = 0 // The Sigmoid Function outputs 0.5 when z = 0\n",
    "\n",
    "So,\n",
    "\n",
    "x1 + x2 = 1.5\n",
    "\n",
    "or\n",
    "\n",
    "x2 = 1.5 - x1 i.e. y = mx + c\n",
    "```\n",
    "\n",
    "Because this equation defines a line in the 2D space (x1, x2) that separates the two classes (0 and 1).\n",
    "\n",
    "**Image**\n",
    "\n",
    "<img src='./Notes_Images/boundary_line.png'>\n",
    "\n",
    "### **Note**\n",
    "\n",
    "We just witnessed how a `Simple Single Perceptron` can model the behavior of an `AND` gate using a non-linear activation function.\n",
    "\n",
    "Now, imagine what `1000s` or `even millions` of these simple perceptrons can achieve when combined in a multi-layer architecture.\n",
    "\n",
    "Also, note that here we witnessed that for the `Non-Linear Activation Function` to give correct output, the combination of `Weights` and `Bias` should be carefully chosen.\n",
    "\n",
    "Therefore, the design of neural networks involves not just the architecture (how many layers, how many neurons per layer) but also the careful tuning of these parameters to achieve the desired performance.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The generalization rule for the `AND` becomes:\n",
    "\n",
    "The weights `w1` and `w2` should be positive and the bias `b` should be negative. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "But,\n",
    "\n",
    "The `Bias` should be a negative number that is bigger than the weighted sum of the inputs when they are both `1`. This ensures that the perceptron will only activate (output 1) when both inputs are 1.\n",
    "\n",
    "**Would `Sigmoid` be able to Generalize well, if the Weights and Bias are not carefully chosen?**\n",
    "\n",
    "No, `Sigmoid` would not be able to generalize well if the weights and bias are not carefully chosen. This is because the `Sigmoid` function is sensitive to the input values, and if the weights and bias do not create a suitable decision boundary, the output may not correctly represent the underlying data distribution.\n",
    "\n",
    "If the `Weights` i.e. `{10,10}` and `Bias` i.e. `{-5}` then the output of the `sigmoid` would be as below:\n",
    "\n",
    "```bash\n",
    "\n",
    "Gate: AND\n",
    "0 AND 0 = 0.00669285 i.e. 0 which is correct\n",
    "0 AND 1 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 0 = 0.993307 i.e. 1 which is not correct should be 0\n",
    "1 AND 1 = 1 i.e. 1 which is correct\n",
    "\n",
    "```\n",
    "\n",
    "Therefore, for the `Non-Linear` Activation function to work effectively, the weights and bias must be chosen carefully to create a suitable decision boundary.\n",
    "\n",
    "For that, Gradient Descent is often used to optimize the weights and bias during the training process.\n",
    "\n",
    "### **Follow Up Questions**\n",
    "\n",
    "**What if our inputs are not binary (0 or 1) but continuous values? How would that affect the design of the perceptron?**\n",
    "\n",
    "**What if we want to implement a different logical operation, such as OR or XOR? How would the design of the perceptron change in those cases?**\n",
    "\n",
    "**What if we want to implement a multi-class classification problem? How would the design of the perceptron change in that case?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a4624",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42e5c3",
   "metadata": {},
   "source": [
    "## **Our Perceptron as an AND Gate**\n",
    "\n",
    "Now let's try to implement our Perceptron as an AND gate. The AND gate outputs 1 only if both inputs are 1, otherwise, it outputs 0.\n",
    "\n",
    "```C++\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "using namespace std;\n",
    "\n",
    "int main()\n",
    "{\n",
    "  Perceptron p(2); // Object with 2 inputs on the Stack, No need to delete\n",
    "\n",
    "  p.set_weights({10, 10, -15}); // +1 Bias\n",
    "}\n",
    "\n",
    "cout << \"Gate: AND\" << endl;\n",
    "\n",
    "cout << \"0 AND 0 = \" << p.run({0,0}) << endl;\n",
    "cout << \"0 AND 1 = \" << p.run({0,1}) << endl;\n",
    "cout << \"1 AND 0 = \" << p.run({1,0}) << endl;\n",
    "cout << \"1 AND 1 = \" << p.run({1,1}) << endl;\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: AND\n",
    "// 0 AND 0 = 3.05902e-07\n",
    "// 0 AND 1 = 0.00669285\n",
    "// 1 AND 0 = 0.00669285\n",
    "// 1 AND 1 = 0.993307\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60017613",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89035573",
   "metadata": {},
   "source": [
    "## **OR Gate**\n",
    "\n",
    "<img src='./Notes_Images/or_gate.png'>\n",
    "\n",
    "The OR gate outputs 1 if at least one of the inputs is 1, otherwise, it outputs 0.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The weights should be `{15,15}` and the bias should be `-10`.\n",
    "\n",
    "The linear combination for the OR gate can be expressed as:\n",
    "\n",
    "```bash\n",
    "15x + 15y - 10 = 0\n",
    "\n",
    "then\n",
    "\n",
    "y = -x + 2/3\n",
    "\n",
    "```\n",
    "\n",
    "Below are the outputs of Sigmoid function for the OR gate:\n",
    "\n",
    "```C++\n",
    "\n",
    "// Output\n",
    "\n",
    "// Gate: OR\n",
    "// 0 AND 0 = 4.53979e-05\n",
    "// 0 AND 1 = 0.993307\n",
    "// 1 AND 0 = 0.993307\n",
    "// 1 AND 1 = 1\n",
    "\n",
    "```\n",
    "\n",
    "**OR Gate Boundary Line Equation**\n",
    "\n",
    "<img src='./Notes_Images/or_gate_boundary.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab66ef",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807be646",
   "metadata": {},
   "source": [
    "## **Linear Separability**\n",
    "\n",
    "`Linear separability` is a property of a dataset that allows it to be separated into different classes using a `linear boundary`. In the context of neural networks, this means that a single layer perceptron can be used to classify the data points.\n",
    "\n",
    "For example, the `AND` gate is linearly separable because we can draw a `straight line` (or `hyperplane` in higher dimensions) that separates the positive examples (1s) from the negative examples (0s).\n",
    "\n",
    "Similarly, the `OR` gate is also linearly separable for the same reason.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- Both the `Straight Line` i.e. `y = mx + b` and the `Hyperplane` i.e. `W*x + b = 0` can be used to separate linearly separable data.\n",
    "\n",
    "- For 2 dimensional data i.e. 2 input features we need the equation of line, for 3 dimensional data, we need the equation of plane i.e. `Ax + By + Cz + D = 0` and for data whose dimension is greater than 3, we need the equation of hyperplane i.e. `W*x + b = 0`, where `W` is the weight vector and `b` is the bias.\n",
    "\n",
    "<hr>\n",
    "\n",
    "On the other hand, the `XOR` gate is not linearly separable because there is no single straight line that can separate the positive examples from the negative examples.\n",
    "\n",
    "Below is the Graphical representation of the `XOR` gate:\n",
    "\n",
    "<img src='./Notes_Images/xor_gate.png'>\n",
    "\n",
    "Here, we cannot separate the positive and negative examples with a single straight line. We would need two lines to separate the classes.\n",
    "\n",
    "If we use an `OR` gate only, it will get all but one of the `XOR` gate inputs correct. The `OR` gate will output `1` for both `(0, 1)` and `(1, 0)` inputs, which is incorrect for the `XOR` operation.\n",
    "\n",
    "<img src='./Notes_Images/or_for_xor.png'>\n",
    "\n",
    "If we use `NAND` gate, it will give one incorrect output for the `(0, 0)` input, which is the only case where the `XOR` gate outputs `1`.\n",
    "\n",
    "<img src='./Notes_Images/nand_for_xor.png'>\n",
    "\n",
    "**But**\n",
    "\n",
    "If we combine the outputs of the `NAND` gate and the `OR` gate, we can create a circuit that correctly implements the `XOR` function.\n",
    "\n",
    "<img src='./Notes_Images/nand_or_xor.png'>\n",
    "\n",
    "### **Creating XOR with NAND, AND, and OR Gates**\n",
    "\n",
    "To create an `XOR` gate using `NAND`, `AND`, and `OR` gates, we can use the following configuration:\n",
    "\n",
    "1. **Inputs**: A and B\n",
    "\n",
    "2. **NAND Gate**: Connect A and B to a `NAND` gate. This will give us the output `NAND(A, B)`.\n",
    "\n",
    "3. **OR Gate**: Connect A and B to an `OR` gate. This will give us the output `OR(A, B)`.\n",
    "\n",
    "4. **AND Gate**: Connect the outputs of the `NAND` gate and the `OR` gate to an `AND` gate. This will give us the final output `XOR(A, B)`.\n",
    "\n",
    "The logical expression for the `XOR` gate can be represented as:\n",
    "\n",
    "```bash\n",
    "XOR(A, B) = AND(NAND(A, B), OR(A, B))\n",
    "```\n",
    "\n",
    "This configuration allows us to implement the `XOR` function using only `NAND`, `AND`, and `OR` gates.\n",
    "\n",
    "**XOR Diagram**\n",
    "\n",
    "<img src='./Notes_Images/xor_diagram.png'>\n",
    "\n",
    "### **Neural Network for XOR Gate**\n",
    "\n",
    "We know a single perceptron can solve a linear separable problem, but the `XOR` function is not linearly separable. Therefore, we need a neural network with `3 perceptrons` i.e. `2 in the hidden layer and 1 in the output layer`.\n",
    "\n",
    "Also, we know that a single perceptron can represent all the three basic logic gates: `AND`, `OR`, and `NAND` each having different `weight` and `bias` configurations.\n",
    "\n",
    "**Linear Equations for Basic Logic Gates**\n",
    "\n",
    "`OR Gate` : `y = -x + 0.5` with Weights `{15,15}` and Bias `{-10}`\n",
    "\n",
    "`NAND Gate` : `y = x + 1.5` with Weights `{-10,-10}` and Bias `{15}`\n",
    "\n",
    "Then we plug the output of `OR Gate` and `NAND Gate` as an Input for the `AND Gate`\n",
    "\n",
    "`AND Gate` : `y = -x + 1.5` with Weights `{10.10}` and Bias `{-15}`\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11968e3",
   "metadata": {},
   "source": [
    "## **Multi Layer Perceptron**\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a type of neural network that consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. MLPs are capable of learning complex patterns in data and can be used for a variety of tasks, including classification and regression.\n",
    "\n",
    "**Image of MLP for XOR Gate**\n",
    "\n",
    "<img src='./Notes_Images/mlp.png'/>\n",
    "\n",
    "### **Architecture of MLP for XOR Gate**\n",
    "\n",
    "1. **Input Layer**: The input layer consists of two inputs, each representing one of the input features (X1 and X2) of the XOR gate.\n",
    "\n",
    "2. **Hidden Layer**: The hidden layer contains two neurons. One of the neurons represents the `NAND Gate` operation, while the other represents the `OR` operation.\n",
    "\n",
    "3. **Output Layer**: The output layer consists of a single neuron that produces the final output of the network. This neuron receives inputs from both hidden layer neurons, applies a weighted sum and a non-linear activation function, and produces the final output (Y) of the `XOR` gate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bf1a7",
   "metadata": {},
   "source": [
    "## **XOR Gate Implementation**\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the `mlp.h` file previously we had written declaration for the `Single Perceptron` class. Now, we will extend this class to create a `MultiLayerPerceptron` class that can handle the XOR problem.\n",
    "\n",
    "### **`mlp.h`**\n",
    "\n",
    "```C++\n",
    "#pragma once\n",
    "\n",
    "#include <algorithm>\n",
    "#include <vector>\n",
    "#include <iostream>\n",
    "#include <random>\n",
    "#include <numeric>\n",
    "#include <cmath>\n",
    "#include <time.h>\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  // Run the Perceptron\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  // Set the Customize Weights if Needed\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  // Sigmoid Activation Function\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "\n",
    "class MultiLayerPerceptron\n",
    "{\n",
    "public:\n",
    "  // Constructor for initilizing layers\n",
    "  MultiLayerPerceptron(std::vector<size_t> layers, double bias = 1.0, double eta = 0.5);\n",
    "\n",
    "  // Set custom weights, w_init for weights of 3 perceptron\n",
    "  void set_weights(std::vector<std::vector<std::vector<double>>> w_init);\n",
    "\n",
    "  // Display the weights\n",
    "  void print_weights();\n",
    "\n",
    "  // Run the MLP\n",
    "  std::vector<double> run(std::vector<double> x);\n",
    "\n",
    "  double bp(std::vector<double> x, std::vector<double> y);\n",
    "\n",
    "  // Attributes\n",
    "\n",
    "  std::vector<size_t> layers; // Unsigned Integers, Number of Neurons Per Layer, 0 for Input, 2 for Hidden, 1 for Output\n",
    "\n",
    "  double bias; // Bias\n",
    "  double eta;  // Learning Rate\n",
    "\n",
    "  std::vector<std::vector<Perceptron>> network;\n",
    "  // 'network' (the outer vector object) is created on the stack.\n",
    "  // But the actual Perceptrons stored inside inner vectors\n",
    "  // will be allocated dynamically on the heap.\n",
    "\n",
    "  // becuase the outer vector is an Object created on the Stack without the New Keyword\n",
    "\n",
    "  std::vector<std::vector<double>> values; // Output values of each neuron in each layer\n",
    "  // Outer vector object is on stack; inner vectors manage heap-allocated arrays for neuron outputs\n",
    "\n",
    "  // becuase the outer vector is an Object created on the Stack without the New Keyword\n",
    "\n",
    "  std::vector<std::vector<double>> d; // Error terms (deltas) for each neuron\n",
    "  // Outer vector object is on stack; inner vectors manage heap-allocated arrays for errors\n",
    "\n",
    "  // becuase the outer vector is an Object created on the Stack without the New Keyword\n",
    "};\n",
    "```\n",
    "\n",
    "In the above code,\n",
    "\n",
    "The `MultiLayerPerceptron` class is designed to handle the XOR problem by utilizing multiple layers of neurons.\n",
    "\n",
    "**`MultiLayerPerceptron(std::vector<size_t> layers, double bias = 1.0, double eta = 0.5)`** : This constructor initializes the MLP with the specified layer structure, bias, and learning rate. It creates the necessary layers and populates the network with `Perceptron` objects.\n",
    "\n",
    "**`run(std::vector<double> x)`** : This method takes an input vector `x` and passes it through the network, returning the output of the MLP.\n",
    "\n",
    "**`bp(std::vector<double> x, std::vector<double> y)`** : This method performs backpropagation to update the weights of the network based on the error between the predicted output and the true output `y`.\n",
    "\n",
    "**`std::vector<std::vector<Perceptron>> network;`** : This attribute holds the layers of the MLP, where each layer is a vector of `Perceptron` objects.\n",
    "\n",
    "**`std::vector<std::vector<double>> values;`** : This attribute stores the output values of each neuron in the network for a given input.\n",
    "\n",
    "**`std::vector<std::vector<double>> d;`** : This attribute holds the error terms for the neurons, which are used during backpropagation to update the weights.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **`mlp.cpp`**\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "/*\n",
    "Single Layer Perceptron Implementation\n",
    "*/\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "\n",
    "/*\n",
    "Multi Layer Perceptron Implementation\n",
    "*/\n",
    "\n",
    "// Return a new MLP Object with the Specified number layers, bias and Learning Rate\n",
    "\n",
    "MultiLayerPerceptron::MultiLayerPerceptron(std::vector<size_t> layers, double bias, double eta) : layers(layers), bias(bias), eta(eta)\n",
    "{\n",
    "  // Create Neurons Layer By Layer\n",
    "  // Outer Loop\n",
    "  for (size_t i = 0; i < layers.size(); i++)\n",
    "  {\n",
    "    // Add Vector of Values Filled with Zeros\n",
    "    values.push_back(vector<double>(layers[i], 0.0)); // Output of Each Neuron Value set to Zero based on the number of Neurons in Each layer\n",
    "\n",
    "    // Add Vector of Neurons\n",
    "    network.push_back(vector<Perceptron>()); // Creates a temporary empty std::vector<Perceptron> object and pushes it into 'network'. Without '()', we would only refer to the type name, not an object, causing a compiler error.\n",
    "\n",
    "    // Inner Loop\n",
    "    // network[0] is the input layer, so it has no neurons\n",
    "    if (i > 0)\n",
    "    {\n",
    "      // Iterate on Each Neuron in the Layer\n",
    "      for (size_t j = 0; j < layers[i]; j++)\n",
    "      {\n",
    "        // Add Perceptron in Every Layer, Starting with the Layer 1, cause 0 is Input Layer\n",
    "        // Each Perceptron Should Accept the Input as Number of Neurons in the Pervious Layer\n",
    "        network[i].push_back(Perceptron(layers[i - 1], bias));\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Here, in the above code for `MLP`,\n",
    "\n",
    "**`MultiLayerPerceptron::MultiLayerPerceptron(std::vector<size_t> layers, double bias, double eta)`** : This constructor initializes the MLP with the specified layer structure, bias, and learning rate. It creates the necessary layers and populates the network with `Perceptron` objects.\n",
    "\n",
    "```C++\n",
    "\n",
    "// Create Neurons Layer By Layer\n",
    "\n",
    "for (size_t i = 0; i < layers.size(); i++)\n",
    "{\n",
    "  // Add Vector of Values Filled with Zeros\n",
    "  values.push_back(vector<double>(layers[i], 0.0));\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The above code creates the necessary layers for the MLP by adding vectors of zeros for each layer.\n",
    "\n",
    "This `vector<double>(layers[i], 0.0)` calls the `std::vector` constructor to create a vector of the specified size (`layers[i]`) initialized with zeros (`0.0`).\n",
    "\n",
    "Which means that the values vector will hold `{{0,0}}` in the first iteration.\n",
    "\n",
    "```C++\n",
    "// Add Vector of Neurons\n",
    "network.push_back(vector<Perceptron>()); // Empty Vector of Perceptrons is added to the end\n",
    "```\n",
    "\n",
    "The above code adds a new vector of `Perceptron` objects for each layer in the network. This sets up the structure for the neurons in each layer, which will be initialized with random weights during the training process.\n",
    "\n",
    "Before `network.push_back(vector<Perceptron>());` the size of `network.size()` is `0`, but after this line, the size becomes `1`.\n",
    "\n",
    "Meaning, for the first iteration i.e. input layer, it will create an empty vector of `Perceptron` objects i.e. `network = {{}}`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "```C++\n",
    "if (i > 0)\n",
    "  {\n",
    "  // Iterate on Each Neuron in the Layer\n",
    "  for (size_t j = 0; j < layers[i]; j++)\n",
    "  {\n",
    "    // Add Perceptron in Every Layer, Starting with the Layer 1, cause 0 is Input Layer\n",
    "    // Each Perceptron Should Accept the Input as Number of Neurons in the Pervious Layer\n",
    "    network[i].push_back(Perceptron(layers[i - 1], bias));\n",
    "  }\n",
    "  }\n",
    "```\n",
    "\n",
    "In the above code,\n",
    "\n",
    "First will be adding `Perceptron` objects to the first hidden layer (layer 1) of the network not the input layer. Therefore, we've `i > 0` condition.\n",
    "\n",
    "Then we will start to add the required number of `Perceptron` objects to the current layer (layer `i`) based on the specified architecture in the `layers` vector.\n",
    "\n",
    "Note: Each `Perceptron` in the current layer will be initialized with the number of inputs equal to the number of neurons in the previous layer, and the specified bias value. `layers[i - 1]` means the number of neurons in the previous layer (layer `i - 1`).\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "**set_weights(vector<vector<vector<double>>> w_init) method**\n",
    "\n",
    "```C++\n",
    "// Set Custom Weights\n",
    "void MultiLayerPerceptron::set_weights(vector<vector<vector<double>>> w_init)\n",
    "{\n",
    "  // Write all the weights into the neural network\n",
    "  // w_init is a vector of vectors of vectors of doubles\n",
    "  for (size_t i = 0; i < w_init.size(); i++)\n",
    "  {\n",
    "    for (size_t j = 0; j < w_init[i].size(); j++)\n",
    "    {\n",
    "      network[i + 1][j].set_weights(w_init[i][j]);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Example Weights Initialization\n",
    "\n",
    "  // Example of w_init\n",
    "  vector<vector<vector<double>>> w = {\n",
    "      {\n",
    "        {0, 0}, {0, 0}\n",
    "      },\n",
    "      {\n",
    "        {0, 0}\n",
    "      }\n",
    "  };\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "In the above code,\n",
    "\n",
    "As our `Constructor` would have initialized,\n",
    "\n",
    "`values` as : `{{0,0}.{0,0},{0}}`\n",
    "\n",
    "and\n",
    "\n",
    "`network` as : `{{},{Perceptron(2,bias),Perceptron(2,bias)},{Perceptron(2,bias)}}`, we can set the weights of the `Perceptron` objects in the network using the `set_weights` method.\n",
    "\n",
    "For example, if the\n",
    "\n",
    "```C++\n",
    "std::vector<std::vector<std::vector<double>>> w_init = {\n",
    "{   // Layer 0 → Hidden\n",
    "  {20.0, 20.0},   // Hidden neuron 1\n",
    "  {-20.0, -20.0}   // Hidden neuron 2\n",
    "},\n",
    "{   // Layer 1 → Output\n",
    "  {20.0, 20.0}    // Output neuron\n",
    "}\n",
    "};\n",
    "```\n",
    "\n",
    "At first `w_init.size()` will be 2. Then we will need to iterate over `Perceptron` of the 1st Layer i.e. `w_init[0].size` is will be 2 i.e. weights for 2 `Perceptron` of the first layer.\n",
    "\n",
    "Then to add the weight in the `Neural Network` starting from the `index 1` as `0 index` does not contain any `Perceptron`.\n",
    "\n",
    "To add the weight in the `Neural Network`, we will use the `set_weights(vector<double> w_init)` method of each `Perceptron` object in the network.\n",
    "\n",
    "For example, for the `Perceptrons` of first layer,\n",
    "\n",
    "`network[1][0].set_weights(w_init[0][0]);` i.e. First `Perceptron` of Layer 1\n",
    "\n",
    "`network[1][1].set_weights(w_init[0][1]);` i.e. Second `Perceptron` of Layer 1\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "**print_weights() method**\n",
    "\n",
    "```C++\n",
    "// Print the Weights\n",
    "void MultiLayerPerceptron::print_weights()\n",
    "{\n",
    "  cout << endl;\n",
    "\n",
    "  for (size_t i = 1; i < network.size(); i++)\n",
    "  {\n",
    "    for (size_t j = 0; j < layers[i]; j++)\n",
    "    {\n",
    "      cout << \"Layer\" << i + 1 << \" Neuron \" << j << \": \";\n",
    "      for (auto &it : network[i][j].weights)\n",
    "      {\n",
    "        cout << it << \" \";\n",
    "      }\n",
    "      cout << endl;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "In the above code, we are printing the weights of each neuron in the network. We iterate through each layer and each neuron within that layer, printing the weights associated with each neuron.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "**`run(vector<double> x)`**\n",
    "\n",
    "```C++\n",
    "// Run the Network\n",
    "vector<double> MultiLayerPerceptron::run(vector<double> x)\n",
    "{\n",
    "  // Run an Input Forward Through the Neural Network\n",
    "  // x is a vector with the input values\n",
    "\n",
    "  // Set the values for the first layer to the given value x, before it was initialized with {0,0}\n",
    "  values[0] = x;\n",
    "\n",
    "  for (size_t i = 1; i < network.size(); i++)\n",
    "  {\n",
    "    for (size_t j = 0; j < layers[i]; j++)\n",
    "    {\n",
    "      values[i][j] = network[i][j].run(values[i - 1]);\n",
    "    }\n",
    "  }\n",
    "  return values.back(); // Return the output of the last layer\n",
    "}\n",
    "```\n",
    "\n",
    "In the above code,\n",
    "\n",
    "`values[0] = x`\n",
    "\n",
    "Here, the initial value of `values` will be `{{0,0},{0,0},{0,0}}`. Then we will set the input values for the first layer to the given value `x`, effectively initializing the first layer with the input values i.e. `values = {{input_vector}, {0,0}, {0,0}}`\n",
    "\n",
    "We then loop the network i.e. from `1 to network.size() - 1` because the first layer is input layer and we need to process the hidden and output layer.\n",
    "\n",
    "Then from each layer we will update the output of each `Perceptron` using the `run` method with the output of the previous layer as input.\n",
    "\n",
    "The `run` function will compute the weighted sum of the inputs and apply the activation i.e. `Sigmoid` and return the output which will be stored as `values[i][j]`.\n",
    "\n",
    "This way all the outputs from the previous layer are used as inputs for the next layer, allowing the network to learn complex patterns in the data.\n",
    "\n",
    "Finally, we return the output of the last layer using `return values.back();`, which gives us the final output of the MLP for the given input `x`. Because the last value of `values` is the output from the last layer of the network.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "### **`neuralNet.cpp`**\n",
    "\n",
    "Now will implement the complete code for `XOR` Gate.\n",
    "\n",
    "```C++\n",
    "\n",
    "// XOR Gate\n",
    "  // Instaintiate with 2 inputs, 2 Perceptron in hidden and 1 Perceptron in the Output\n",
    "  MultiLayerPerceptron mlp({2, 2, 1});\n",
    "\n",
    "  // Set the Weights, NAND Gate, OR Gate and then AND Gate\n",
    "  mlp.set_weights(\n",
    "      {{{-10, -10, 15}, {15, 15, -10}},\n",
    "       {{10, 10, -15}}});\n",
    "\n",
    "  cout << endl;\n",
    "\n",
    "  // Print Weights\n",
    "  cout << \"Hardcoded Weights:\" << endl;\n",
    "  mlp.print_weights();\n",
    "\n",
    "  cout << endl;\n",
    "\n",
    "  // Run the Network\n",
    "  cout << \"XOR: \" << endl;\n",
    "  cout << \"0 0 = \" << mlp.run({0, 0})[0] << endl; // For 0 0 Input, Output should be  0.00669585\n",
    "  cout << \"0 0 = \" << mlp.run({0, 1})[0] << endl; // For 0 1 Input, Output should be  1\n",
    "  cout << \"0 0 = \" << mlp.run({1, 0})[0] << endl; // For 1 0 Input, Output should be  1\n",
    "  cout << \"0 0 = \" << mlp.run({1, 1})[0] << endl; // For 1 1 Input, Output should be  0\n",
    "\n",
    "// Output\n",
    "\n",
    "// Hardcoded Weights:\n",
    "\n",
    "// Layer2 Neuron 0: -10 -10 15\n",
    "// Layer2 Neuron 1: 15 15 -10\n",
    "// Layer3 Neuron 0: 10 10 -15\n",
    "\n",
    "// XOR:\n",
    "// 0 0 = 0.00669585\n",
    "// 0 0 = 0.992356\n",
    "// 0 0 = 0.992356\n",
    "// 0 0 = 0.00715281\n",
    "```\n",
    "\n",
    "In the above code, we are implementing a Multi-Layer Perceptron (MLP) to solve the XOR problem. The MLP consists of an input layer, a hidden layer, and an output layer. We hardcode the weights for the neurons in each layer to mimic the behavior of the XOR gate. The `print_weights()` method displays the weights of each neuron, and the `run()` method takes an input vector and computes the output of the network by propagating the inputs through the layers. Finally, we test the network with all possible combinations of binary inputs for the XOR gate and print the results.\n",
    "\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28401114",
   "metadata": {},
   "source": [
    "## **Mathematical Intution Behind XOR Gate**\n",
    "\n",
    "<hr>\n",
    "\n",
    "First, let's look at our Neural Network,\n",
    "\n",
    "Hidden 1 (NAND): $(-10,-10; \\, +15)$\n",
    "\n",
    "Hidden 2 (OR): $(+15,+15; \\, -10)$\n",
    "\n",
    "Output (AND): $(+10,+10; \\, -15)$\n",
    "\n",
    "Let $\\sigma(t)=\\dfrac{1}{1+e^{-t}}$.\n",
    "\n",
    "# Hidden layer\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_1 &= -10x_1-10x_2+15,\n",
    "&h_1 &= \\sigma(z_1),\\\\[2pt]\n",
    "z_2 &= \\phantom{-}15x_1+15x_2-10,\n",
    "&h_2 &= \\sigma(z_2).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "# Output neuron\n",
    "\n",
    "$$\n",
    "z_{\\text{out}} = 10h_1+10h_2-15,\\qquad\n",
    "y=\\sigma(z_{\\text{out}}).\n",
    "$$\n",
    "\n",
    "# Explicit closed-form (final equation)\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\n",
    "y(x_1,x_2)=\\sigma\\!\\Big(\n",
    "10\\,\\sigma(-10x_1-10x_2+15)\\;+\\;10\\,\\sigma(15x_1+15x_2-10)\\;-\\;15\n",
    "\\Big)\\; }\n",
    "$$\n",
    "\n",
    "# Decision boundaries (hyperplanes at $\\sigma=0.5$)\n",
    "\n",
    "- Hidden 1 (NAND): $z_1=0 \\iff \\boxed{x_1+x_2=1.5}$.\n",
    "- Hidden 2 (OR): $z_2=0 \\iff \\boxed{x_1+x_2=\\tfrac{10}{15}=\\tfrac{2}{3}}$.\n",
    "- Output (in $(h_1,h_2)$-space): $z_{\\text{out}}=0 \\iff \\boxed{h_1+h_2=1.5}$.\n",
    "\n",
    "Pulled back to $(x_1,x_2)$-space, the **final decision boundary** is the implicit curve\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\\sigma(-10x_1-10x_2+15)\\;+\\;\\sigma(15x_1+15x_2-10)\\;=\\;1.5\\;}\n",
    "$$\n",
    "\n",
    "(Region “$y>0.5$” is where the left-hand side $>\\,1.5$.)\n",
    "\n",
    "<hr>\n",
    "\n",
    "But how?\n",
    "\n",
    "### 1. Reminder: what happens for AND\n",
    "\n",
    "For the AND gate with weights $(10,10,-15)$, the **decision boundary** was\n",
    "\n",
    "$$\n",
    "\\sigma(10x_1+10x_2-15)=0.5 \\;\\;\\Longleftrightarrow\\;\\; 10x_1+10x_2-15=0,\n",
    "$$\n",
    "\n",
    "because $\\sigma(z)=0.5 \\iff z=0$. That’s a straight line.\n",
    "\n",
    "So a **single perceptron** always produces a _linear_ boundary.\n",
    "\n",
    "### 2. XOR with 2 hidden neurons\n",
    "\n",
    "Now for XOR, the **output neuron** takes two hidden activations:\n",
    "\n",
    "$$\n",
    "h_1 = \\sigma(-10x_1 - 10x_2 + 15), \\qquad\n",
    "h_2 = \\sigma(15x_1 + 15x_2 - 10).\n",
    "$$\n",
    "\n",
    "Then at the output:\n",
    "\n",
    "$$\n",
    "z_{\\text{out}} = 10h_1 + 10h_2 - 15, \\quad\n",
    "y=\\sigma(z_{\\text{out}}).\n",
    "$$\n",
    "\n",
    "The **decision boundary** is $y=0.5 \\iff z_{\\text{out}}=0$:\n",
    "\n",
    "$$\n",
    "10h_1 + 10h_2 - 15 = 0\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "h_1 + h_2 = 1.5.\n",
    "$$\n",
    "\n",
    "### 3. Why it’s nonlinear in $(x_1, x_2)$\n",
    "\n",
    "Substitute the hidden activations:\n",
    "\n",
    "$$\n",
    "\\sigma(-10x_1-10x_2+15) + \\sigma(15x_1+15x_2-10) = 1.5.\n",
    "$$\n",
    "\n",
    "Each term here is a **sigmoid of a linear function**.\n",
    "\n",
    "- A sigmoid of a line is an **S-shaped curve**, not a line.\n",
    "- The **sum of two sigmoids = constant** is generally a _curved contour_.\n",
    "\n",
    "That’s why the boundary is **not linear** anymore—it bends.\n",
    "\n",
    "### 4. Visual intuition\n",
    "\n",
    "- For a **single perceptron**, the boundary is always a straight line, because the “0.5 cutoff” corresponds to the input = 0.\n",
    "\n",
    "- But once you **stack perceptrons** (like in XOR), the output depends on hidden nonlinearities. The “boundary = 0.5” condition then becomes an implicit equation involving **sums of sigmoids**, which defines a **nonlinear curve**.\n",
    "\n",
    "- In the **steep-weight limit**, the sigmoids approximate steps. Then the boundary becomes a polygon-shaped region:\n",
    "\n",
    "  $$\n",
    "  \\tfrac{2}{3} < x_1+x_2 < 1.5,\n",
    "  $$\n",
    "\n",
    "  which is not a line, but the _intersection of two half-planes_ → that’s what makes XOR work.\n",
    "\n",
    "So, the reason it’s “not a line” is because once you include multiple hidden sigmoids, the boundary equation is no longer linear in $(x_1, x_2)$; it’s the **level set of a sum of nonlinear functions**.\n",
    "\n",
    "Below is the **plot this curve** (showing how it bends between the four XOR points), so you see exactly why it isn’t a line?\n",
    "\n",
    "<img src='./Notes_Images/xor_graph.png'>\n",
    "\n",
    "**Desmos for XOR**\n",
    "\n",
    "<img src='./Notes_Images/desmos_xor.png'>\n",
    "\n",
    "This shows that `XOR` needs two half-planes rather than one. And the **decision boundary** is the intersection of these two half-planes not a single straight line which means we found the non-linear decision boundary.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Exact numeric evaluations on $\\{0,1\\}^2$\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "(x_1,x_2) & (z_1,h_1) & (z_2,h_2) & z_{\\text{out}} & y=\\sigma(z_{\\text{out}})\\\\ \\hline\n",
    "(0,0) & (15,\\;0.9999996941) & (-10,\\;4.5398{\\times}10^{-5}) & -4.99954908 & 0.00669585\\\\\n",
    "(0,1) & (5,\\;0.9933071491) & (5,\\;0.9933071491) & \\phantom{-}4.86614298 & 0.99235586\\\\\n",
    "(1,0) & (5,\\;0.9933071491) & (5,\\;0.9933071491) & \\phantom{-}4.86614298 & 0.99235586\\\\\n",
    "(1,1) & (-5,\\;0.0066928509) & (20,\\;0.9999999980) & -4.93307151 & 0.00715281\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(Threshold $0.5$ yields XOR: $0,1,1,0$.)\n",
    "\n",
    "# Why the solution is not linear\n",
    "\n",
    "- Each $h_i=\\sigma(a_ix_1+b_ix_2+c_i)$ is **nonlinear** in $(x_1,x_2)$.\n",
    "- The final boundary is the **nonlinear** implicit curve\n",
    "  $\\sigma(-10x_1-10x_2+15)+\\sigma(15x_1+15x_2-10)=1.5$ (not a line).\n",
    "- In the large-slope limit, $\\sigma$ behaves like a Heaviside step, giving the band\n",
    "  $\\tfrac{2}{3}<x_1+x_2<1.5$ as the positive region—again **not** a single hyperplane.\n",
    "\n",
    "These equations fully specify the XOR realized by your MLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf51aae",
   "metadata": {},
   "source": [
    "## **Why Training is Needed in the MLP?**\n",
    "\n",
    "If we've noticed,\n",
    "\n",
    "Until this point we've been using hardcoded weights for our neurons in the Multi-Layer Perceptron (MLP). This approach allows us to quickly test the network's behavior without going through the training process. However, hardcoding weights is not a scalable solution for more complex problems or larger datasets.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Also, for small problems like solving `AND`, `OR` and `XOR` functions we can use other alternatives as well i.e. `Programming Logic` instead of `Neural Network`.\n",
    "\n",
    "Because, the real value of `Neural Network` is its ability to learn complex patterns and generalize from examples, making it suitable for a wide range of tasks beyond simple logic functions.\n",
    "\n",
    "Instead of hardcoding weights, how about we show a lots of examples of how an `XOR` behaves so that it can learn from those examples?\n",
    "\n",
    "For that we use an algorithm called `backpropagation`, which is a supervised learning algorithm used for training artificial neural networks.\n",
    "\n",
    "### **Reasons to Train a Neural Network**\n",
    "\n",
    "**Linear Separability Is Hardly a Given**\n",
    "\n",
    "In many real-world problems, the data is not linearly separable. This means that a simple linear decision boundary (a straight line in 2D, for example) cannot effectively separate the different classes in the data. Neural networks, especially those with hidden layers, can learn complex, non-linear decision boundaries, making them much more powerful for a wide range of tasks.\n",
    "\n",
    "Take the below example:\n",
    "\n",
    "Suppose we're trying to classify `Small or Large` based on `Length` and `Width`. Below is the scatter plot of the data points:\n",
    "\n",
    "<img src=\"./Notes_Images/l_w.png\" alt=\"Scatter Plot\">\n",
    "\n",
    "Here, we can see that the closest we can draw a line that classifies with less error is the `line` i.e `Single Perceptron` shown in the graph. We've misclassified some points, indicating that a linear boundary is not sufficient for this problem.\n",
    "\n",
    "To improve, we can use `MLP` to generalize the decision boundary and better classify the data points.\n",
    "\n",
    "Below is the graph:\n",
    "\n",
    "<img src=\"./Notes_Images/mlp_db.png\" alt=\"Scatter Plot\">\n",
    "\n",
    "We can see here, we are only misclassifying one data point. It's better but not perfect, and that's what we are looking for.\n",
    "\n",
    "That is the whole point of training a neural network: to minimize misclassifications and improve accuracy.\n",
    "\n",
    "<hr>\n",
    "\n",
    "There are three situations:\n",
    "\n",
    "**Underfitting** occurs when the model is too simple to capture the underlying patterns in the data. This can happen if the model has too few parameters or if it is not trained long enough. Underfitting results in high training and validation errors.\n",
    "\n",
    "**Overfitting** occurs when the model is too complex and learns the noise in the training data instead of the actual patterns. For the training data, this means the model performs well (low error) but fails to generalize to new, unseen data (high error).\n",
    "\n",
    "Or for the data that are close to the decision boundary, the model may become overly sensitive to small fluctuations in the input, leading to erratic predictions.\n",
    "\n",
    "**Good fitting** occurs when the model is able to generalize well to unseen data. This is the desired outcome of the training process, where the model achieves a balance between bias and variance.\n",
    "\n",
    "<img src='./Notes_Images/triad_nn.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Datasets**\n",
    "\n",
    "For training we need data and dataset is a collection of examples used to train the model. Each example consists of input features and the corresponding target output.\n",
    "\n",
    "We teach the network by showing samples to it. The `Neural Network` learns with each feature-label pair.\n",
    "\n",
    "We split the datasets into three parts:\n",
    "\n",
    "**Training** : This subset is used to train the model. The model learns from this data by adjusting its weights based on the input-output pairs.\n",
    "\n",
    "**Validation** : This subset is used to tune the model's hyperparameters and prevent overfitting. The model is evaluated on this data during training, but it does not learn from it.\n",
    "\n",
    "**Testing** : This subset is used to assess the model's performance after training is complete. The model makes predictions on this data, and its accuracy is measured.\n",
    "\n",
    "<hr>\n",
    "\n",
    "So we the data with which the `Network` learns is only the `Training` set. Other subsets are used asserting the model's performance and generalization ability.\n",
    "\n",
    "We've to run the `Training` set lots of times to allow the model to learn effectively. Each such run is called an `Epoch`. We stop after some number of `Epochs` when we see that the model's performance on the validation set is no longer improving.\n",
    "\n",
    "After running `Training` dataset for some `epochs` the `Network` would have learned something therefore, we can evaluate the model's performance on the `Validation` set to see how well it generalizes to unseen dat and compared to other competitors.\n",
    "\n",
    "Also, while `Training` the data we use multiple `Architecture` and `Techniques`. The best performing model on the validation set is selected for further evaluation on the test set.\n",
    "\n",
    "<img src='./Notes_Images/rank.png'>\n",
    "\n",
    "Lastly, the model's performance on the test set is evaluated to get an unbiased estimate of its generalization ability. This is crucial for understanding how well the model will perform in real-world scenarios.\n",
    "\n",
    "### **What Happens When we Train One Single Training Sample?**\n",
    "\n",
    "First we feed an input sample `X` to the `Network`.\n",
    "\n",
    "Then compare the output to the correct value of `Y`.\n",
    "\n",
    "Calculate the error.\n",
    "\n",
    "Use this error to adjust the weights of the network through backpropagation.\n",
    "\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786e17",
   "metadata": {},
   "source": [
    "## **Training Error Functions**\n",
    "\n",
    "An error function measures how bad a model's predictions are compared to the actual target values. It quantifies the difference between the predicted output and the true output, providing a way to assess the model's performance during training.\n",
    "\n",
    "The `Error Function` is a crucial component in training neural networks, as it guides the optimization process. By minimizing the error function, we can improve the model's predictions and overall performance.\n",
    "\n",
    "We use a training process called `Gradient Descent` to minimize the error function.\n",
    "\n",
    "Also, we use two error metrics i.e. the `Output Error` for the individual predictions and the `Overall Error` which is also known as the `Loss Function`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Output Error()**\n",
    "\n",
    "The **Output Error** measures the difference between the predicted output of the network and the actual target output for a single training example. It provides a way to assess how well the network is performing on individual predictions.\n",
    "\n",
    "Mathematically, the output error for a single example can be defined as:\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} (y - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(y\\) is the true output (target value)\n",
    "- $\\hat{y}$ is the predicted output (network's output)\n",
    "\n",
    "The factor of $\\tfrac{1}{2}$ is included for convenience, as it simplifies the derivative calculation during backpropagation.\n",
    "\n",
    "The output error is used to compute the gradients for updating the weights in the network. By minimizing the output error for each training example, we can improve the network's performance on that specific example.\n",
    "\n",
    "### **Overall Error (Loss Function)**\n",
    "\n",
    "The **Overall Error**, also known as the **Loss Function**, measures the average error across all training examples. It provides a single scalar value that represents the model's performance on the entire training dataset.\n",
    "\n",
    "Mathematically, the overall error can be defined as:\n",
    "\n",
    "Or\n",
    "\n",
    "`Mean Squared Error (MSE)`\n",
    "\n",
    "$$\n",
    "E_{overall} = \\frac{1}{N} \\sum_{i=1}^{N} E_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `N` is the total number of training examples\n",
    "- \\(E_i\\) is the output error for the \\(i\\)-th training example\n",
    "\n",
    "The overall error is used to guide the optimization process during training. By minimizing the overall error, we can improve the model's performance across all training examples.\n",
    "\n",
    "<hr>\n",
    "\n",
    "The advantage of using `Mean Squared Error (MSE)` as the overall error metric is that it penalizes larger errors more heavily than smaller ones. This is because the errors are squared before being averaged, which means that outliers have a greater impact on the overall error value. This property makes MSE a useful metric for tasks where large errors are particularly undesirable.\n",
    "\n",
    "Also, MSE is differentiable, which is a key requirement for optimization algorithms like gradient descent. The smooth nature of the MSE curve allows for more stable and efficient convergence during training.\n",
    "\n",
    "Also, `MSE` gets rid of `Sign` of the actual error. So, when minimizing the error, the model does not have to worry about the direction of the error (positive or negative), only the magnitude.\n",
    "\n",
    "The main purpose is to find how big or small the error is, regardless of its direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f4ab8",
   "metadata": {},
   "source": [
    "### **Delta Rule**\n",
    "\n",
    "It is a simple update formula for adjusting the weights in a neuron. We add the product of the learning rate, the error term, and the input to the weight.\n",
    "\n",
    "**Considers the Following Value**\n",
    "\n",
    "- The output error\n",
    "\n",
    "- One Input\n",
    "\n",
    "- Learning rate factor.\n",
    "\n",
    "Mathematically, it can be expressed as:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\eta \\cdot (d - y) \\cdot x\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ \\Delta w $ is the change in weight\n",
    "\n",
    "- $ \\eta $ is the learning rate\n",
    "\n",
    "- $ d $ is the desired output\n",
    "\n",
    "- $ y $ is the actual output\n",
    "\n",
    "- $ x $ is the input\n",
    "\n",
    "This rule helps the model learn from its mistakes by adjusting the weights in the direction that reduces the error.\n",
    "\n",
    "**Output Error**\n",
    "\n",
    "The `output error` will be `positive` if the predicted output is higher than the desired output and `negative` if it is lower. This means that when we later update the `Weight`, it will contribute to making the output closer to the desired value.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- **The Delta Rule is a _special case_ of Gradient Descent.**\n",
    "- Specifically, it’s (stochastic) gradient descent on a **single linear neuron** using **squared-error loss**.\n",
    "\n",
    "Below is the clean picture,\n",
    "\n",
    "Assume a single neuron with linear activation:\n",
    "$y = w^\\top x$\n",
    "\n",
    "Use the per-example squared error:\n",
    "$E = \\tfrac12(d - y)^2$\n",
    "\n",
    "Take the gradient w\\.r.t. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w}\n",
    "= \\frac{\\partial}{\\partial w}\\tfrac12(d - w^\\top x)^2\n",
    "= -(d - y)\\,x\n",
    "$$\n",
    "\n",
    "One **gradient descent** step does:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial E}{\\partial w}\n",
    "= w + \\eta(d - y)\\,x\n",
    "$$\n",
    "\n",
    "which is exactly the **Delta Rule** you wrote:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\eta(d - y)\\,x.\n",
    "$$\n",
    "\n",
    "So, the delta rule = “do (stochastic) gradient descent on a linear unit with MSE.”\n",
    "\n",
    "Gradient Descent is the general recipe:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta\\,\\frac{\\partial E_{\\text{overall}}}{\\partial w}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- the **model** can be anything differentiable (linear, logistic, deep nets, …),\n",
    "- the **loss** can be anything differentiable (MSE, cross-entropy, etc.),\n",
    "- and you can update **per example** (SGD), **per mini-batch**, or **full-batch**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0bd3f",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9e904",
   "metadata": {},
   "source": [
    "### **Gradient Descent**\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the error function by iteratively updating the model's parameters (weights and biases). The basic idea is to compute the gradient (or derivative) of the error function with respect to the model's parameters and then adjust the parameters in the opposite direction of the gradient.\n",
    "\n",
    "The update rule for a single parameter \\(w\\) can be expressed as:\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial E_{overall}}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ w $ is the model parameter (weight or bias)\n",
    "\n",
    "- $ \\eta\\ $ is the learning rate (a small positive scalar)\n",
    "\n",
    "- $ \\frac{\\partial E\\_{overall}}{\\partial w}\\ $ is the gradient of the overall error with respect to the parameter \\(w\\)\n",
    "\n",
    "The learning rate controls the step size of the update. If the learning rate is too large, the optimization process may overshoot the minimum and diverge. If it is too small, the convergence may be slow.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Understanding with Graph**\n",
    "\n",
    "Suppose we've a Network with several weights but for now let's try to study how changing single weight affects the overall error.\n",
    "\n",
    "<img src='./Notes_Images/gradient_des.png'>\n",
    "\n",
    "In the graph above, say at any point in training the Weight \\(w\\) is at position \\(x\\) on the x-axis. The corresponding overall error is at position \\(y\\) on the y-axis. As we adjust the weight \\(w\\), we can see how the overall error changes.\n",
    "\n",
    "We see that we'll need to increase the weight \\(w\\) to reduce the overall error \\(E\\_{overall}\\) i.e. we reach the `Global Minimum`.\n",
    "\n",
    "This is the lowest error we can get by modifying the weight \\(w\\) in the direction that reduces the overall error.\n",
    "\n",
    "**But,**\n",
    "\n",
    "What if the `Weight` is at a Local Minimum? As show in the image below:\n",
    "\n",
    "<img src='./Notes_Images/local_minimum.png'>\n",
    "\n",
    "As we will initialize the weights randomly, there's a possibility that the weight \\(w\\) could start at a local minimum. In such cases, the gradient descent algorithm may not be able to escape the `local minimum` and find the global minimum.\n",
    "\n",
    "<hr>\n",
    "\n",
    "This was the case for a `Single Weight`, but in real neural networks, we have many weights and biases. The optimization landscape becomes much more complex, with many local minima and saddle points. This makes it challenging for gradient descent to find the global minimum.\n",
    "\n",
    "For example, let's say we are modifying two weights to manipulate the error. This would give us a `3D` plot where the height is the error and `two weights` will place the marble at different points in this surface with mountains and valley. The object is to find the lowest point in this 3D landscape, which corresponds to the minimum error.\n",
    "\n",
    "Below is the `Graph`:\n",
    "\n",
    "<img src='./Notes_Images/3d_grad.png'>\n",
    "\n",
    "It becomes more complex as we add more weights and biases, creating a high-dimensional optimization landscape. In this space, the gradient descent algorithm must navigate through various local minima and saddle points to find the global minimum.\n",
    "\n",
    "So, with `Two Weights` it became a `3D` optimization problem. Similiarly, for our `XOR` problem we would have a `10D` optimization problem as we've `9 Weights` which we cannot event understand graphically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb64e01",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bfc5a0",
   "metadata": {},
   "source": [
    "## **Types of Gradient Descent**\n",
    "\n",
    "There are different types of `Gradient Descent` algorithms, each with its own characteristics and use cases:\n",
    "\n",
    "1. **Batch Gradient Descent**: Computes the gradient using the entire dataset. It provides a stable estimate of the gradient but can be slow and memory-intensive for large datasets.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**: Computes the gradient using a single randomly chosen example. It is faster and can escape local minima but introduces noise into the optimization process.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**: Combines the advantages of both batch and stochastic gradient descent by using a small random subset (mini-batch) of the data to compute the gradient. It strikes a balance between speed and stability.\n",
    "\n",
    "4. **Momentum**: An extension of SGD that accumulates a velocity vector in the direction of the gradient, helping to accelerate convergence and reduce oscillations.\n",
    "\n",
    "5. **Adaptive Learning Rate Methods**: Algorithms like AdaGrad, RMSProp, and Adam adjust the learning rate for each parameter based on past gradients, allowing for more efficient training.\n",
    "\n",
    "6. **Nesterov Accelerated Gradient (NAG)**: A variant of momentum that looks ahead at the future position of the parameters, providing a more accurate gradient estimate.\n",
    "\n",
    "Each of these methods has its own strengths and weaknesses, and the choice of which to use depends on the specific problem and dataset.\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c762919",
   "metadata": {},
   "source": [
    "## **Backpropagation**\n",
    "\n",
    "`Backpropagation` is a supervised learning algorithm used for training artificial neural networks. It is a form of gradient descent that computes the gradient of the `loss function` with respect to each `weight` by the chain rule, allowing for efficient weight updates.\n",
    "\n",
    "This algorithm will update the `Weights` throughout the network.\n",
    "\n",
    "<hr>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
