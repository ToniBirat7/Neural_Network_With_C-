{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd15eedf",
   "metadata": {},
   "source": [
    "## **Resources**\n",
    "\n",
    "### **Training Neural Network with C++**\n",
    "\n",
    "[Linkedin_Learning](https://www.linkedin.com/learning/training-neural-networks-in-c-plus-plus-22661958/the-many-applications-of-machine-learning?autoSkip=true&resume=false&u=42288921)\n",
    "\n",
    "### **Understanding Neural Network in Depth**\n",
    "\n",
    "[Essential_Idea_Of_Neural_Network](https://www.youtube.com/watch?v=CqOfi41LfDw)\n",
    "\n",
    "[How_CNN_Works_in_Depth](https://www.youtube.com/watch?v=JB8T_zN7ZC0)\n",
    "\n",
    "### **The Mathematics Behind Neural Network**\n",
    "\n",
    "[Maths_Behind_Neural_Network](https://www.youtube.com/watch?v=Ixl3nykKG9M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61e01b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b162004",
   "metadata": {},
   "source": [
    "## **Neural Network Implementation Note**\n",
    "\n",
    "- All values must be real numbers, not integers. We will use double point precision (e.g., 0.1, 0.2).\n",
    "\n",
    "- The weights and inputs may be implemented as `1-D` vectors. We will use the `std::vector<double>` type from the C++ Standard Library i.e. `vec(w)` and `vec(x)`.\n",
    "\n",
    "- This way, the sum may be calculated in one operation: `z = vec(w) * vec(x)`.\n",
    "\n",
    "- We will feed the weighted sum to the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85211873",
   "metadata": {},
   "source": [
    "## **Files and Their Meaning**\n",
    "\n",
    "**`.h` files**: These are header files in C++ that typically contain function declarations, class definitions, and macros. They are included in `.cpp` files to provide the necessary declarations for the functions and classes used in the implementation.\n",
    "\n",
    "**`.cpp` files**: These are source files in C++ that contain the actual implementation of the functions and classes declared in the corresponding header files. They are compiled to create the final executable program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74332d50",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2128f",
   "metadata": {},
   "source": [
    "## **Neural Network into Action**\n",
    "\n",
    "We will write all the declarations in the header files and all the implementations in the source files. This will help us keep our code organized and modular.\n",
    "\n",
    "Our first task it to implement basic `Multi Layer Perceptron` class in C++.\n",
    "\n",
    "For that we are creating `MLP.h` and `MLP.cpp` files.\n",
    "\n",
    "### **`MLP.h`**\n",
    "\n",
    "```C++\n",
    "\n",
    "// Perceptron class\n",
    "\n",
    "class Perceptron\n",
    "{\n",
    "public:\n",
    "  std::vector<double> weights;\n",
    "  double bias;\n",
    "\n",
    "  // Constructor\n",
    "  Perceptron(size_t inputs, double bias = 1.0);\n",
    "\n",
    "  double run(std::vector<double> x);\n",
    "\n",
    "  void set_weights(std::vector<double> w_init);\n",
    "\n",
    "  double sigmoid(double x);\n",
    "};\n",
    "```\n",
    "\n",
    "Here, `size_t` is used to represent the number of inputs to the perceptron, ensuring that the value is always non-negative. It is an `unsigned integer` type which store `8 Bytes` in 64 Bit System and `4 Bytes` in 32 Bit System.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now we'll implement the `Perceptron` class in the `MLP.cpp`.\n",
    "\n",
    "### **`MLP.cpp`**\n",
    "\n",
    "Here, we will write the implementation of the `Perceptron` class.\n",
    "\n",
    "```C++\n",
    "\n",
    "#include \"mlp.h\"\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "// Random Number Generator Function\n",
    "\n",
    "double frand()\n",
    "{\n",
    "  return (2.0 * (double)rand() / RAND_MAX) - 1.0;\n",
    "}\n",
    "\n",
    "// Return a new Perceptron Object with the Specified number of Inputs (+1 for the bias)\n",
    "\n",
    "Perceptron::Perceptron(size_t inputs, double bias)\n",
    "{\n",
    "  this->bias = bias;\n",
    "\n",
    "  // Initialize the Weights as Random numbers of Double between -1 and 1\n",
    "\n",
    "  weights.resize(inputs + 1); // Resize the Vector for Weights + Bias\n",
    "\n",
    "  // Generate Random Numbers and Fill in the Vectors. Pass the frand function to generate the number\n",
    "\n",
    "  generate(weights.begin(), weights.end(), frand);\n",
    "}\n",
    "\n",
    "// Run Function\n",
    "// Feeds an Input Vector X into the perceptron to return the activation function output.\n",
    "\n",
    "double Perceptron::run(std::vector<double> x)\n",
    "{\n",
    "\n",
    "  // Add the bias at the end\n",
    "  x.push_back(bias);\n",
    "\n",
    "  // Weighted Sum\n",
    "  double sum = inner_product(x.begin(), x.end(), weights.begin(), (double)0.0);\n",
    "\n",
    "  return sigmoid(sum); // Pass into the sigmoid function\n",
    "}\n",
    "\n",
    "// Set the weights. w_init is a vector with the Weights\n",
    "\n",
    "void Perceptron::set_weights(std::vector<double> w_init)\n",
    "{\n",
    "  weights = w_init; // Copies the vector\n",
    "}\n",
    "\n",
    "// Evaluate the Sigmoid Function for the floating point of input\n",
    "\n",
    "double Perceptron::sigmoid(double x)\n",
    "{\n",
    "  return 1.0 / (1.0 + exp(-x));\n",
    "}\n",
    "```\n",
    "\n",
    "Below is the Step wise Step Explanation for Each Implementation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
