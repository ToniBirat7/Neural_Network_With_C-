{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd346528",
   "metadata": {},
   "source": [
    "# **Resources**\n",
    "\n",
    "## **Neural Networks**\n",
    "\n",
    "### **Complete Mathematics of Neural Network**\n",
    "\n",
    "[Adam_Dhalla_Best](https://www.youtube.com/watch?v=Ixl3nykKG9M&t=2s)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Mathematics**\n",
    "\n",
    "### **Multivariable Calculus**\n",
    "\n",
    "[IIT_Rookie](https://www.youtube.com/playlist?list=PL1XTxGlLddCzE1mHJVul22iGYE5NrSB3I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b64c71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d59cf",
   "metadata": {},
   "source": [
    "# **Complete Mathematics of Neural Network**\n",
    "\n",
    "**course Syllabus**\n",
    "\n",
    "<img src='./Notes_Images/syllabus.png'>\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "- Basics of Linear Algebra\n",
    "\n",
    "- Multivariable Calculus\n",
    "\n",
    "  - Differential Equations\n",
    "\n",
    "  - Jacobian / Gradients\n",
    "\n",
    "- Base ML Knowledge\n",
    "\n",
    "### **Agenda**\n",
    "\n",
    "- **Big Picture of Neural Networks**\n",
    "\n",
    "- **Multivariable Calculus Refresher**\n",
    "\n",
    "- **Neuron as Function**\n",
    "\n",
    "- **Jacobians and Neural Networks**\n",
    "\n",
    "- **Gradient Descent**\n",
    "\n",
    "- **Backpropagation**\n",
    "\n",
    "- **Backpropagation as Matrix Multiplication**\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "<img src='./Notes_Images/notation.png'>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a22048",
   "metadata": {},
   "source": [
    "## **Neural Networks as Functions**\n",
    "\n",
    "Neural Network is a big fancy function that is made up of many smaller functions (neurons) that work together to transform input data into output predictions.\n",
    "\n",
    "All the parameters to this functions are the weights and biases of the neurons, which are learned during training.\n",
    "\n",
    "Input to this function is a `vector` of features, and the output is a `vector` of predictions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Neural Network` is just a `Big Calculus Problem` in which we are trying to minimize the difference between the predicted output and the actual output (loss function) by adjusting the weights and biases using optimization techniques like gradient descent.\n",
    "\n",
    "It's all about finding the `Derivative` of `Loss Function` w.r.t `Weights` and `Biases` (parameters of the model) and using that information to update the parameters in the direction that reduces the loss.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ L $ is the loss function\n",
    "\n",
    "- $ \\hat{y} $ is the predicted output\n",
    "\n",
    "- $ w $ is the weight\n",
    "\n",
    "This is just the chain rule from calculus, and it allows us to compute the gradient of the loss function with respect to the weights and biases of the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Say we've a function with thousands parameters, and to figure out how to lower that loss we'll have to find out how much each `Weight` and `Bias` contributes to the loss. So that we can add or substract some value to our initial `Weights` to lower the `Loss`.\n",
    "\n",
    "This is done with `Backpropagation` and `Gradient Descent`.\n",
    "\n",
    "This means we need to find $ \\frac{\\partial L}{\\partial w_i} $ for each `Weight` $ w_i $ in the model.\n",
    "\n",
    "This is where automatic differentiation comes in handy. It allows us to compute these gradients efficiently without having to derive them manually.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## **Multivariable Calculus Refresher**\n",
    "\n",
    "### **Understand the Meaning of `Gradients`**\n",
    "\n",
    "In `Univariate Calculus`, the derivative of a function at a point gives us the `slope of the tangent line to the function at that point`. This tells us how much the function is changing at that point.\n",
    "\n",
    "For example, let's understand mathematically:\n",
    "\n",
    "Let $f(x) = x^2$. The derivative of this function is $f'(x) = 2x$. This tells us that at any point $x$, the slope of the tangent line to the function is $2x$. If we evaluate this at $x=1$, we find that the slope is $2$. This means that the function is increasing at that point.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In `Multivariable Calculus`, we extend this concept to functions of multiple variables. The gradient of a function is a `vector` that contains all of its `partial derivatives`. It points in the `direction` of the `steepest ascent` of the function.\n",
    "\n",
    "Mathematically, if we have a function $f(x, y)$, the gradient is given by:\n",
    "\n",
    "$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) $$\n",
    "\n",
    "This vector points in the direction of the steepest increase of the function.\n",
    "\n",
    "**Image Notes**\n",
    "\n",
    "<img src='./Notes_Images/note1.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note2.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note3.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note4.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Worked Examples to Understand `Gradient` and it's Significance**\n",
    "\n",
    "**Example A — linear approximation and directional derivative**\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "f(x,y) = x^2 + 4y^2.\n",
    "$$\n",
    "\n",
    "Compute $\\nabla f(x,y) = (2x, 8y)$. At point $(1,1)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(1,1) = (2,8).\n",
    "$$\n",
    "\n",
    "Magnitude:\n",
    "\n",
    "$$\n",
    "\\|\\nabla f\\| = \\sqrt{2^2 + 8^2} = \\sqrt{4 + 64} = \\sqrt{68}.\n",
    "$$\n",
    "\n",
    "We can keep the exact $\\sqrt{68}$ or approximate: $\\sqrt{68}\\approx 8.246211\\ldots$ — so the steepest rate of increase is about $8.246$ units of $f$ per unit distance in $(x,y)$-space.\n",
    "\n",
    "If we move $\\Delta\\mathbf{x}=(0.01,0.005)$, linear prediction:\n",
    "\n",
    "$$\n",
    "\\Delta f \\approx \\nabla f\\cdot \\Delta\\mathbf{x} = 2\\cdot 0.01 + 8\\cdot 0.005 = 0.02 + 0.04 = 0.06.\n",
    "$$\n",
    "\n",
    "Actual $f(1.01,1.005)$ equals\n",
    "\n",
    "$$\n",
    "(1.01)^2 + 4(1.005)^2 = 1.0201 + 4\\cdot 1.010025 = 1.0201 + 4.0401 = 5.0602,\n",
    "$$\n",
    "\n",
    "initial $f(1,1)=5$. Actual change $=0.0602$, close to linear prediction $0.06$. This demonstrates the gradient gives a very good first-order prediction.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example B — gradient descent in 2D (one optimization step)**\n",
    "\n",
    "Minimize\n",
    "\n",
    "$$\n",
    "f(x,y)=(x-3)^2 + 2(y+1)^2.\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla f = (2(x-3),\\; 4(y+1)).\n",
    "$$\n",
    "\n",
    "Start at $(0,0)$. Compute gradient at $(0,0)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(0,0) = (2(0-3), 4(0+1)) = (-6, 4).\n",
    "$$\n",
    "\n",
    "Use learning rate $\\eta=0.1$. Gradient descent update:\n",
    "\n",
    "$$\n",
    "(x_{\\text{new}},y_{\\text{new}}) = (0,0) - 0.1\\cdot(-6,4) = (0+0.6,\\; 0-0.4) = (0.6,\\,-0.4).\n",
    "$$\n",
    "\n",
    "Function values:\n",
    "\n",
    "$$\n",
    "f(0,0) = 9 + 2 = 11.\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(0.6,-0.4) = (0.6-3)^2 + 2(-0.4+1)^2 = (-2.4)^2 + 2(0.6)^2 = 5.76 + 2\\cdot0.36 = 5.76 + 0.72 = 6.48.\n",
    "$$\n",
    "\n",
    "Big drop from $11$ to $6.48$ in one step — the gradient told us a good descent direction.\n",
    "\n",
    "Second step (quick):\n",
    "\n",
    "$$\n",
    "\\nabla f(0.6,-0.4)= (2(0.6-3), 4(-0.4+1)) = (-4.8, 2.4).\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "(0.6,-0.4) - 0.1(-4.8,2.4) = (1.08,\\,-0.64).\n",
    "$$\n",
    "\n",
    "Value $f(1.08,-0.64)=3.9456$ — decreasing further.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Gradients` is the transformation of `Vector` to `Scalar` fields, providing a way to understand how functions change in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2090",
   "metadata": {},
   "source": [
    "## **Understanding `Jacobian` Chain Rule**\n",
    "\n",
    "[Understand_Jacobian](https://www.youtube.com/watch?v=wCZ1VEmVjVo)\n",
    "\n",
    "Watch the above video to have complete idea of `Matrix`, `Vectors` and `Jacobian`.\n",
    "\n",
    "### **What is `Jacobian`?**\n",
    "\n",
    "The `Jacobian` is a matrix that contains all the first-order partial derivatives of a vector-valued function. If we have a function $\\mathbf{f} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix is an $m \\times n$ matrix given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Jacobian` is another way of representing `Partial Derivatives` of a function that transforms a `vector input` into a `vector output`.\n",
    "\n",
    "For example, if we've a function `f: R^2 -> R^2` defined as:\n",
    "\n",
    "$$\n",
    "f(x,y) = \\begin{bmatrix}\n",
    "x^2 + y \\\\\n",
    "2x + 3y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix is given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "So, first we take the `Partial Derivative` for the first function i.e. `f_1(x,y) = x^2 + y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_1}{\\partial x} = 2x, \\quad \\frac{\\partial f_1}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "Next, for the second function i.e. `f_2(x,y) = 2x + 3y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_2}{\\partial x} = 2, \\quad \\frac{\\partial f_2}{\\partial y} = 3\n",
    "$$\n",
    "\n",
    "Putting it all together, the Jacobian matrix is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "2x & 1 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Jacobian` is the matrix representing best `Linear Map` approximation of a function `f: R^n -> R^m` at a given point `(a,b)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8fcdb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72173255",
   "metadata": {},
   "source": [
    "# **Understanding `Derivative` and `Gradient` in Detail**\n",
    "\n",
    "## **Why do we find gradients?** (start from 1-D intuition)\n",
    "\n",
    "- **In 1-D:** the derivative $f'(x)$ at a point $x=a$ is the _instantaneous rate of change_ — the slope of the tangent line to the graph at $a$.\n",
    "  That slope answers: _if I change the input by a tiny amount $h$, how much does the output change, to first order?_\n",
    "  Concretely,\n",
    "\n",
    "  $$\n",
    "  f(a+h)\\approx f(a) + f'(a)\\,h \\qquad\\text{for small }h.\n",
    "  $$\n",
    "\n",
    "  So we find derivatives because they give a **linear prediction** of how the function responds to small inputs. That prediction is cheap to compute and extremely useful in practice (physics, economics, optimization, error estimates).\n",
    "\n",
    "- **Simple 1-D example (step-by-step arithmetic):**\n",
    "  Take $f(x)=x^2$. Then $f'(x)=2x$.\n",
    "  At $x=3$: $f'(3)=2\\times 3 = 6$.\n",
    "  If $h=0.01$ then linear prediction:\n",
    "\n",
    "  $$\n",
    "  f(3+0.01)\\approx f(3)+f'(3)\\cdot 0.01 = 9 + 6\\cdot0.01 = 9 + 0.06 = 9.06.\n",
    "  $$\n",
    "\n",
    "  Exact value: $f(3.01) = (3.01)^2 = 9.0601.$\n",
    "  Error $= 9.0601 - 9.06 = 0.0001$, which is $\\mathcal{O}(h^2)$. The derivative gave us the dominant (linear) change.\n",
    "\n",
    "- **Why that matters (practical reasons):**\n",
    "\n",
    "  - Predict local behavior without evaluating the full nonlinear formula.\n",
    "  - Drive optimization (find minima/maxima).\n",
    "  - Model instantaneous rates (velocity = derivative of position).\n",
    "  - Build numerical solvers (Newton’s method linearizes with derivatives).\n",
    "\n",
    "---\n",
    "\n",
    "### **If the function is “higher order” (e.g. a polynomial), doesn’t the derivative become a polynomial too — so what does the derivative _at a point_ represent?**\n",
    "\n",
    "Short answer: **The derivative of a polynomial is indeed another polynomial, but the value of that derivative at a specific point is still the instantaneous slope at that point.** Distinguish between (a) the _derivative function_ and (b) the _derivative evaluated at a point_.\n",
    "\n",
    "- **Derivative function vs derivative at a point**\n",
    "\n",
    "  - The derivative $f'(x)$ is a function (it may be polynomial if $f$ is polynomial).\n",
    "\n",
    "  - The number $f'(a)$ (plugging $x=a$) is the slope at $x=a$. That number is what we use in the linear approximation at that point.\n",
    "\n",
    "- **Worked example (step-by-step):**\n",
    "  Let $p(x)=x^3+2x$. Then compute derivative:\n",
    "\n",
    "  $$\n",
    "  p'(x)=3x^2+2.\n",
    "  $$\n",
    "\n",
    "  At $x=2$: evaluate derivative\n",
    "\n",
    "  $$\n",
    "  p'(2) = 3\\times 2^2 + 2 = 3\\times 4 + 2 = 12 + 2 = 14.\n",
    "  $$\n",
    "\n",
    "  So the slope at $x=2$ is $14$. The tangent line at $x=2$ is given with `point-slope` form:\n",
    "\n",
    "  $$\n",
    "  y - y_1 = m(x - x_1)\n",
    "  $$\n",
    "\n",
    "  Which becomes below for $x=2$:\n",
    "\n",
    "  $$\n",
    "  y \\approx p(2) + p'(2)(x-2).\n",
    "  $$\n",
    "\n",
    "  Where,\n",
    "\n",
    "  $$\n",
    "  y_1 = p(2), \\quad m = p'(2), \\quad x_1 = 2.\n",
    "  $$\n",
    "\n",
    "  Now,\n",
    "\n",
    "  Compute $p(2)=2^3+2\\times2=8+4=12$. So tangent ≈ $12 + 14(x-2)$.\n",
    "  For $h=0.01$ we predict\n",
    "\n",
    "  $$\n",
    "  p(2+0.01)\\approx 12 + 14\\cdot 0.01 = 12 + 0.14 = 12.14.\n",
    "  $$\n",
    "\n",
    "  Exact: $(2.01)^3 = 8.120601$ (because $2.01^2 = 4.0401$, times $2.01$ gives $8.120601$), and $2\\cdot2.01 = 4.02$, so\n",
    "\n",
    "  $$\n",
    "  p(2.01) = 8.120601 + 4.02 = 12.140601.\n",
    "  $$\n",
    "\n",
    "  Error $= 12.140601 - 12.14 = 0.000601 $, again $\\mathcal{O}(h^2)$.\n",
    "\n",
    "- **Interpretation:** even for high-degree polynomials, the derivative evaluated at a point is the linear coefficient of the local (first-order) approximation. Higher derivatives (second, third, …) measure curvature, cubic bending, etc., and appear in higher-order Taylor terms:\n",
    "\n",
    "  $$\n",
    "  f(a+h)=f(a)+f'(a)h+\\tfrac{1}{2}f''(a)h^2+\\cdots.\n",
    "  $$\n",
    "\n",
    "  So what the derivative at a point is called: _instantaneous rate of change_, _slope of tangent_, or _first-order coefficient_ of the Taylor expansion.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is the _gradient_ required? Wasn't the derivative enough?**\n",
    "\n",
    "Now move from single-variable to multivariable. The one-dimensional derivative is **not enough** once you have more than one input variable.\n",
    "\n",
    "- **Problem with a single scalar derivative:** If $f$ depends on many inputs $x_1,\\dots,x_n$, a single number cannot describe how $f$ changes when you vary **each** input independently. You need a collection of partial rates — one per input direction.\n",
    "\n",
    "- **Definition (multivariable):** For $f:\\mathbb{R}^n\\to\\mathbb{R}$, the **gradient**\n",
    "\n",
    "  $$\n",
    "  \\nabla f(x) = \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_1} & \\dfrac{\\partial f}{\\partial x_2} & \\cdots & \\dfrac{\\partial f}{\\partial x_n}\\end{bmatrix}^\\top\n",
    "  $$\n",
    "\n",
    "  is the vector of all first partial derivatives. It generalizes the derivative: the first-order linear approximation becomes\n",
    "\n",
    "  $$\n",
    "  f(x+h)\\approx f(x) + \\nabla f(x)\\cdot h,\n",
    "  $$\n",
    "\n",
    "  where “$\\cdot$” is the dot product. So the gradient is the **best linear predictor** in _every direction_ simultaneously.\n",
    "\n",
    "- **What the gradient tells you:**\n",
    "\n",
    "  - **Direction of steepest increase:** the unit direction $u$ that maximizes the directional derivative $\\nabla f(x)\\cdot u$ is $u = \\nabla f(x)/\\|\\nabla f(x)\\|$.\n",
    "\n",
    "  - **Directional derivative:** the rate of change in direction $u$ is $\\nabla f(x)\\cdot u$.\n",
    "\n",
    "  - **Orthogonality to level sets:** the gradient is perpendicular to level sets (contours) of $f$.\n",
    "\n",
    "- **Concrete multivariable example (step-by-step):**\n",
    "  Let $f(x,y)=x^2 + 3xy + y^2.$ Compute partials:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x} = 2x + 3y,\\qquad\n",
    "  \\frac{\\partial f}{\\partial y} = 3x + 2y.\n",
    "  $$\n",
    "\n",
    "  At the point $(1,1)$:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x}\\Big|_{(1,1)} = 2\\cdot1 + 3\\cdot1 = 2 + 3 = 5,\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial y}\\Big|_{(1,1)} = 3\\cdot1 + 2\\cdot1 = 3 + 2 = 5.\n",
    "  $$\n",
    "\n",
    "  So $\\nabla f(1,1) = \\begin{bmatrix}5\\\\5\\end{bmatrix}.$\n",
    "\n",
    "  - **Directional derivative** in the unit direction $u=\\tfrac{1}{\\sqrt{2}}(1,1)$ is\n",
    "\n",
    "    $$\n",
    "    \\nabla f(1,1)\\cdot u = [5,5]\\cdot\\Big[\\tfrac{1}{\\sqrt2},\\tfrac{1}{\\sqrt2}\\Big] = 5\\cdot\\tfrac{1}{\\sqrt2} + 5\\cdot\\tfrac{1}{\\sqrt2} = \\frac{10}{\\sqrt2}.\n",
    "    $$\n",
    "\n",
    "    Simplify: $\\tfrac{10}{\\sqrt2} = 5\\sqrt2 \\approx 5\\times1.41421356 = 7.0710678.$\n",
    "\n",
    "  That number (≈7.071) is the instantaneous rate of increase of $f$ if we move in the 45° direction from $(1,1)$.\n",
    "\n",
    "- **Why derivative alone would be insufficient:** a single number cannot tell you rates along multiple coordinates or along arbitrary directions. The gradient bundles all per-input sensitivity into one object and allows you to compute directional rates by dotting with the direction vector.\n",
    "\n",
    "- **Use in optimization (practical):** gradient tells us which way to move to decrease the function fastest (take negative gradient). Gradient descent update:\n",
    "\n",
    "  $$\n",
    "  x_{\\text{new}} = x_{\\text{old}} - \\eta\\,\\nabla f(x_{\\text{old}}),\n",
    "  $$\n",
    "\n",
    "  where $\\eta$ is a step size. This is the engine behind most continuous optimization and training in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick hierarchy recap (so the relations are crystal clear)**\n",
    "\n",
    "1. **Derivative (1-D)** = instantaneous slope at a specific point; gives linear (first-order) approximation: $f(a+h)\\approx f(a)+f'(a)h.$\n",
    "\n",
    "2. **Derivative function** (e.g. $f'(x)$ for a polynomial) is itself a function; evaluating it at a point produces the slope at that point.\n",
    "\n",
    "3. **Gradient (multivariable)** = vector of partial derivatives; the direct generalization of derivative when the input is multidimensional. Gives linear approximation in every direction: $f(x+h)\\approx f(x)+\\nabla f(x)\\cdot h.$\n",
    "\n",
    "4. **Higher derivatives (second, Hessian, etc.)** capture curvature and give quadratic (or higher) approximations: $f(a+h)=f(a)+f'(a)h+\\tfrac12 f''(a)h^2+\\cdots$.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
