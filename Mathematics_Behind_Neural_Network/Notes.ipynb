{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd346528",
   "metadata": {},
   "source": [
    "# **Resources**\n",
    "\n",
    "## **Complete Mathematics of Neural Network**\n",
    "\n",
    "[Adam_Dhalla_Best](https://www.youtube.com/watch?v=Ixl3nykKG9M&t=2s)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Mathematics**\n",
    "\n",
    "### **Multivariable Calculus**\n",
    "\n",
    "**IIT Mathematics**\n",
    "\n",
    "Explains Theory, with mathematical proof.\n",
    "\n",
    "[IIT_Rookie](https://www.youtube.com/playlist?list=PL1XTxGlLddCzE1mHJVul22iGYE5NrSB3I)\n",
    "\n",
    "**Dr. Trefor Bazett**\n",
    "\n",
    "Explains with Graphs\n",
    "\n",
    "[Dr_Trefor_Bazett](https://www.youtube.com/playlist?list=PLHXZ9OQGMqxc_CvEy7xBKRQr6I214QJcd)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **YT Channels That Visualize and Explains Mathematics**\n",
    "\n",
    "**Wumbo**\n",
    "\n",
    "He has connection with `3B1B`. He has started to create content related to mathematics.\n",
    "\n",
    "[Wumbo](https://www.youtube.com/@wumbo_dot_net/videos)\n",
    "\n",
    "**3Blue1Brown**\n",
    "\n",
    "[Essence_Of_Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Websites Related to Maths Contribution**\n",
    "\n",
    "**James Schloss**\n",
    "\n",
    "He has connection with `3B1B`. He actively contributes to the [algorithm-archive.org](https://www.algorithm-archive.org/). The `Arcane Algorithm Archive` is a collaborative effort to create a guide for all important algorithms in all languages.\n",
    "\n",
    "We can contribute to this project as well, refer to [how_to_contribute](https://www.algorithm-archive.org/contents/how_to_contribute/how_to_contribute.html)\n",
    "\n",
    "**3Blue1Brown**\n",
    "\n",
    "[Link](https://www.3blue1brown.com/)\n",
    "\n",
    "**3B1B : Summer of Math Exposition**\n",
    "\n",
    "The Summer of Math Exposition (SoME) is an annual competition fostering the creation of excellent math content online. You can participate as either a creator or judge.\n",
    "\n",
    "[Link](https://some.3b1b.co/)\n",
    "\n",
    "I've decided to participate in this competition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b64c71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d59cf",
   "metadata": {},
   "source": [
    "# **Complete Mathematics of Neural Network**\n",
    "\n",
    "**course Syllabus**\n",
    "\n",
    "<img src='./Notes_Images/syllabus.png'>\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "- Basics of Linear Algebra\n",
    "\n",
    "- Multivariable Calculus\n",
    "\n",
    "  - Differential Equations\n",
    "\n",
    "  - Jacobian / Gradients\n",
    "\n",
    "- Base ML Knowledge\n",
    "\n",
    "### **Agenda**\n",
    "\n",
    "- **Big Picture of Neural Networks**\n",
    "\n",
    "- **Multivariable Calculus Refresher**\n",
    "\n",
    "- **Neuron as Function**\n",
    "\n",
    "- **Jacobians and Neural Networks**\n",
    "\n",
    "- **Gradient Descent**\n",
    "\n",
    "- **Backpropagation**\n",
    "\n",
    "- **Backpropagation as Matrix Multiplication**\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "<img src='./Notes_Images/notation.png'>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2aa7f",
   "metadata": {},
   "source": [
    "# **Understanding `Derivative` and `Gradient` in Detail**\n",
    "\n",
    "## **Why do we find gradients?** (start from 1-D intuition)\n",
    "\n",
    "- **In 1-D:** the derivative $f'(x)$ at a point $x=a$ is the _instantaneous rate of change_ — the slope of the tangent line to the graph at $a$.\n",
    "  That slope answers: _if I change the input by a tiny amount $h$, how much does the output change, to first order?_\n",
    "  Concretely,\n",
    "\n",
    "  $$\n",
    "  f(a+h)\\approx f(a) + f'(a)\\,h \\qquad\\text{for small }h.\n",
    "  $$\n",
    "\n",
    "  So we find derivatives because they give a **linear prediction** of how the function responds to small inputs. That prediction is cheap to compute and extremely useful in practice (physics, economics, optimization, error estimates).\n",
    "\n",
    "- **Simple 1-D example (step-by-step arithmetic):**\n",
    "  Take $f(x)=x^2$. Then $f'(x)=2x$.\n",
    "  At $x=3$: $f'(3)=2\\times 3 = 6$.\n",
    "  If $h=0.01$ then linear prediction:\n",
    "\n",
    "  $$\n",
    "  f(3+0.01)\\approx f(3)+f'(3)\\cdot 0.01 = 9 + 6\\cdot0.01 = 9 + 0.06 = 9.06.\n",
    "  $$\n",
    "\n",
    "  Exact value: $f(3.01) = (3.01)^2 = 9.0601.$\n",
    "  Error $= 9.0601 - 9.06 = 0.0001$, which is $\\mathcal{O}(h^2)$. The derivative gave us the dominant (linear) change.\n",
    "\n",
    "- **Why that matters (practical reasons):**\n",
    "\n",
    "  - Predict local behavior without evaluating the full nonlinear formula.\n",
    "  - Drive optimization (find minima/maxima).\n",
    "  - Model instantaneous rates (velocity = derivative of position).\n",
    "  - Build numerical solvers (Newton’s method linearizes with derivatives).\n",
    "\n",
    "---\n",
    "\n",
    "### **If the function is “higher order” (e.g. a polynomial), doesn’t the derivative become a polynomial too — so what does the derivative _at a point_ represent?**\n",
    "\n",
    "Short answer: **The derivative of a polynomial is indeed another polynomial, but the value of that derivative at a specific point is still the instantaneous slope at that point.** Distinguish between (a) the _derivative function_ and (b) the _derivative evaluated at a point_.\n",
    "\n",
    "- **Derivative function vs derivative at a point**\n",
    "\n",
    "  - The derivative $f'(x)$ is a function (it may be polynomial if $f$ is polynomial).\n",
    "\n",
    "  - The number $f'(a)$ (plugging $x=a$) is the slope at $x=a$. That number is what we use in the linear approximation at that point.\n",
    "\n",
    "- **Worked example (step-by-step):**\n",
    "  Let $p(x)=x^3+2x$. Then compute derivative:\n",
    "\n",
    "  $$\n",
    "  p'(x)=3x^2+2.\n",
    "  $$\n",
    "\n",
    "  At $x=2$: evaluate derivative\n",
    "\n",
    "  $$\n",
    "  p'(2) = 3\\times 2^2 + 2 = 3\\times 4 + 2 = 12 + 2 = 14.\n",
    "  $$\n",
    "\n",
    "  So the slope at $x=2$ is $14$. The tangent line at $x=2$ is given with `point-slope` form:\n",
    "\n",
    "  $$\n",
    "  y - y_1 = m(x - x_1)\n",
    "  $$\n",
    "\n",
    "  Which becomes below for $x=2$:\n",
    "\n",
    "  $$\n",
    "  y \\approx p(2) + p'(2)(x-2).\n",
    "  $$\n",
    "\n",
    "  Where,\n",
    "\n",
    "  $$\n",
    "  y_1 = p(2), \\quad m = p'(2), \\quad x_1 = 2.\n",
    "  $$\n",
    "\n",
    "  Now,\n",
    "\n",
    "  Compute $p(2)=2^3+2\\times2=8+4=12$. So tangent ≈ $12 + 14(x-2)$.\n",
    "  For $h=0.01$ we predict\n",
    "\n",
    "  $$\n",
    "  p(2+0.01)\\approx 12 + 14\\cdot 0.01 = 12 + 0.14 = 12.14.\n",
    "  $$\n",
    "\n",
    "  Exact: $(2.01)^3 = 8.120601$ (because $2.01^2 = 4.0401$, times $2.01$ gives $8.120601$), and $2\\cdot2.01 = 4.02$, so\n",
    "\n",
    "  $$\n",
    "  p(2.01) = 8.120601 + 4.02 = 12.140601.\n",
    "  $$\n",
    "\n",
    "  Error $= 12.140601 - 12.14 = 0.000601 $, again $\\mathcal{O}(h^2)$.\n",
    "\n",
    "- **Interpretation:** even for high-degree polynomials, the derivative evaluated at a point is the linear coefficient of the local (first-order) approximation. Higher derivatives (second, third, …) measure curvature, cubic bending, etc., and appear in higher-order Taylor terms:\n",
    "\n",
    "  $$\n",
    "  f(a+h)=f(a)+f'(a)h+\\tfrac{1}{2}f''(a)h^2+\\cdots.\n",
    "  $$\n",
    "\n",
    "  So what the derivative at a point is called: _instantaneous rate of change_, _slope of tangent_, or _first-order coefficient_ of the Taylor expansion.\n",
    "\n",
    "**Visualization:**\n",
    "\n",
    "We can visualize the tangent line and the function it approximates in the actual `Graph` as below:\n",
    "\n",
    "<img src='./Notes_Images/deri.png'>\n",
    "\n",
    "---\n",
    "\n",
    "### **Why is the _gradient_ required? Wasn't the derivative enough?**\n",
    "\n",
    "Now move from single-variable to multivariable. The one-dimensional derivative is **not enough** once you have more than one input variable.\n",
    "\n",
    "- **Problem with a single scalar derivative:** If $f$ depends on many inputs $x_1,\\dots,x_n$, a single number cannot describe how $f$ changes when you vary **each** input independently. You need a collection of partial rates — one per input direction.\n",
    "\n",
    "- **Definition (multivariable):** For $f:\\mathbb{R}^n\\to\\mathbb{R}$, the **gradient**\n",
    "\n",
    "  $$\n",
    "  \\nabla f(x) = \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_1} & \\dfrac{\\partial f}{\\partial x_2} & \\cdots & \\dfrac{\\partial f}{\\partial x_n}\\end{bmatrix}^\\top\n",
    "  $$\n",
    "\n",
    "  is the vector of all first partial derivatives. It generalizes the derivative: the first-order linear approximation becomes\n",
    "\n",
    "  $$\n",
    "  f(x+h)\\approx f(x) + \\nabla f(x)\\cdot h,\n",
    "  $$\n",
    "\n",
    "  where “$\\cdot$” is the dot product. So the gradient is the **best linear predictor** in _every direction_ simultaneously.\n",
    "\n",
    "- **What the gradient tells you:**\n",
    "\n",
    "  - **Direction of steepest increase:** the unit direction $u$ that maximizes the directional derivative $\\nabla f(x)\\cdot u$ is $u = \\nabla f(x)/\\|\\nabla f(x)\\|$.\n",
    "\n",
    "  - **Directional derivative:** the rate of change in direction $u$ is $\\nabla f(x)\\cdot u$.\n",
    "\n",
    "  - **Orthogonality to level sets:** the gradient is perpendicular to level sets (contours) of $f$.\n",
    "\n",
    "- **Concrete multivariable example (step-by-step):**\n",
    "  Let $f(x,y)=x^2 + 3xy + y^2.$ Compute partials:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x} = 2x + 3y,\\qquad\n",
    "  \\frac{\\partial f}{\\partial y} = 3x + 2y.\n",
    "  $$\n",
    "\n",
    "  At the point $(1,1)$:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial x}\\Big|_{(1,1)} = 2\\cdot1 + 3\\cdot1 = 2 + 3 = 5,\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial y}\\Big|_{(1,1)} = 3\\cdot1 + 2\\cdot1 = 3 + 2 = 5.\n",
    "  $$\n",
    "\n",
    "  So $\\nabla f(1,1) = \\begin{bmatrix}5\\\\5\\end{bmatrix}.$\n",
    "\n",
    "  - **Directional derivative** in the unit direction $u=\\tfrac{1}{\\sqrt{2}}(1,1)$ is\n",
    "\n",
    "    $$\n",
    "    \\nabla f(1,1)\\cdot u = [5,5]\\cdot\\Big[\\tfrac{1}{\\sqrt2},\\tfrac{1}{\\sqrt2}\\Big] = 5\\cdot\\tfrac{1}{\\sqrt2} + 5\\cdot\\tfrac{1}{\\sqrt2} = \\frac{10}{\\sqrt2}.\n",
    "    $$\n",
    "\n",
    "    Simplify: $\\tfrac{10}{\\sqrt2} = 5\\sqrt2 \\approx 5\\times1.41421356 = 7.0710678.$\n",
    "\n",
    "  That number (≈7.071) is the instantaneous rate of increase of $f$ if we move in the 45° direction from $(1,1)$.\n",
    "\n",
    "- **Why derivative alone would be insufficient:** a single number cannot tell you rates along multiple coordinates or along arbitrary directions. The gradient bundles all per-input sensitivity into one object and allows you to compute directional rates by dotting with the direction vector.\n",
    "\n",
    "- **Use in optimization (practical):** gradient tells us which way to move to decrease the function fastest (take negative gradient). Gradient descent update:\n",
    "\n",
    "  $$\n",
    "  x_{\\text{new}} = x_{\\text{old}} - \\eta\\,\\nabla f(x_{\\text{old}}),\n",
    "  $$\n",
    "\n",
    "  where $\\eta$ is a step size. This is the engine behind most continuous optimization and training in machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick hierarchy recap (so the relations are crystal clear)**\n",
    "\n",
    "1. **Derivative (1-D)** = instantaneous slope at a specific point; gives linear (first-order) approximation: $f(a+h)\\approx f(a)+f'(a)h.$\n",
    "\n",
    "2. **Derivative function** (e.g. $f'(x)$ for a polynomial) is itself a function; evaluating it at a point produces the slope at that point.\n",
    "\n",
    "3. **Gradient (multivariable)** = vector of partial derivatives; the direct generalization of derivative when the input is multidimensional. Gives linear approximation in every direction: $f(x+h)\\approx f(x)+\\nabla f(x)\\cdot h.$\n",
    "\n",
    "4. **Higher derivatives (second, Hessian, etc.)** capture curvature and give quadratic (or higher) approximations: $f(a+h)=f(a)+f'(a)h+\\tfrac12 f''(a)h^2+\\cdots$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a22048",
   "metadata": {},
   "source": [
    "## **Neural Networks as Functions**\n",
    "\n",
    "Neural Network is a big fancy function that is made up of many smaller functions (neurons) that work together to transform input data into output predictions.\n",
    "\n",
    "All the parameters to this functions are the weights and biases of the neurons, which are learned during training.\n",
    "\n",
    "Input to this function is a `vector` of features, and the output is a `vector` of predictions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Neural Network` is just a `Big Calculus Problem` in which we are trying to minimize the difference between the predicted output and the actual output (loss function) by adjusting the weights and biases using optimization techniques like gradient descent.\n",
    "\n",
    "It's all about finding the `Derivative` of `Loss Function` w.r.t `Weights` and `Biases` (parameters of the model) and using that information to update the parameters in the direction that reduces the loss.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ L $ is the loss function\n",
    "\n",
    "- $ \\hat{y} $ is the predicted output\n",
    "\n",
    "- $ w $ is the weight\n",
    "\n",
    "This is just the chain rule from calculus, and it allows us to compute the gradient of the loss function with respect to the weights and biases of the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Say we've a function with thousands parameters, and to figure out how to lower that loss we'll have to find out how much each `Weight` and `Bias` contributes to the loss. So that we can add or substract some value to our initial `Weights` to lower the `Loss`.\n",
    "\n",
    "This is done with `Backpropagation` and `Gradient Descent`.\n",
    "\n",
    "This means we need to find $ \\frac{\\partial L}{\\partial w_i} $ for each `Weight` $ w_i $ in the model.\n",
    "\n",
    "This is where automatic differentiation comes in handy. It allows us to compute these gradients efficiently without having to derive them manually.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## **Multivariable Calculus Refresher**\n",
    "\n",
    "### **Understand the Meaning of `Gradients`**\n",
    "\n",
    "In `Univariate Calculus`, the derivative of a function at a point gives us the `slope of the tangent line to the function at that point`. This tells us how much the function is changing at that point.\n",
    "\n",
    "For example, let's understand mathematically:\n",
    "\n",
    "Let $f(x) = x^2$. The derivative of this function is $f'(x) = 2x$. This tells us that at any point $x$, the slope of the tangent line to the function is $2x$. If we evaluate this at $x=1$, we find that the slope is $2$. This means that the function is increasing at that point.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In `Multivariable Calculus`, we extend this concept to functions of multiple variables. The gradient of a function is a `vector` that contains all of its `partial derivatives`. It points in the `direction` of the `steepest ascent` of the function.\n",
    "\n",
    "Mathematically, if we have a function $f(x, y)$, the gradient is given by:\n",
    "\n",
    "$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) $$\n",
    "\n",
    "This vector points in the direction of the steepest increase of the function.\n",
    "\n",
    "**Image Notes**\n",
    "\n",
    "<img src='./Notes_Images/note1.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note2.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note3.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note4.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Worked Examples to Understand `Gradient` and it's Significance**\n",
    "\n",
    "**Example A — linear approximation and directional derivative**\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "f(x,y) = x^2 + 4y^2.\n",
    "$$\n",
    "\n",
    "Compute $\\nabla f(x,y) = (2x, 8y)$. At point $(1,1)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(1,1) = (2,8).\n",
    "$$\n",
    "\n",
    "Magnitude:\n",
    "\n",
    "$$\n",
    "\\|\\nabla f\\| = \\sqrt{2^2 + 8^2} = \\sqrt{4 + 64} = \\sqrt{68}.\n",
    "$$\n",
    "\n",
    "We can keep the exact $\\sqrt{68}$ or approximate: $\\sqrt{68}\\approx 8.246211\\ldots$ — so the steepest rate of increase is about $8.246$ units of $f$ per unit distance in $(x,y)$-space.\n",
    "\n",
    "If we move $\\Delta\\mathbf{x}=(0.01,0.005)$, linear prediction:\n",
    "\n",
    "$$\n",
    "\\Delta f \\approx \\nabla f\\cdot \\Delta\\mathbf{x} = 2\\cdot 0.01 + 8\\cdot 0.005 = 0.02 + 0.04 = 0.06.\n",
    "$$\n",
    "\n",
    "Actual $f(1.01,1.005)$ equals\n",
    "\n",
    "$$\n",
    "(1.01)^2 + 4(1.005)^2 = 1.0201 + 4\\cdot 1.010025 = 1.0201 + 4.0401 = 5.0602,\n",
    "$$\n",
    "\n",
    "initial $f(1,1)=5$. Actual change $=0.0602$, close to linear prediction $0.06$. This demonstrates the gradient gives a very good first-order prediction.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example B — gradient descent in 2D (one optimization step)**\n",
    "\n",
    "Minimize\n",
    "\n",
    "$$\n",
    "f(x,y)=(x-3)^2 + 2(y+1)^2.\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla f = (2(x-3),\\; 4(y+1)).\n",
    "$$\n",
    "\n",
    "Start at $(0,0)$. Compute gradient at $(0,0)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(0,0) = (2(0-3), 4(0+1)) = (-6, 4).\n",
    "$$\n",
    "\n",
    "Use learning rate $\\eta=0.1$. Gradient descent update:\n",
    "\n",
    "$$\n",
    "(x_{\\text{new}},y_{\\text{new}}) = (0,0) - 0.1\\cdot(-6,4) = (0+0.6,\\; 0-0.4) = (0.6,\\,-0.4).\n",
    "$$\n",
    "\n",
    "Function values:\n",
    "\n",
    "$$\n",
    "f(0,0) = 9 + 2 = 11.\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(0.6,-0.4) = (0.6-3)^2 + 2(-0.4+1)^2 = (-2.4)^2 + 2(0.6)^2 = 5.76 + 2\\cdot0.36 = 5.76 + 0.72 = 6.48.\n",
    "$$\n",
    "\n",
    "Big drop from $11$ to $6.48$ in one step — the gradient told us a good descent direction.\n",
    "\n",
    "Second step (quick):\n",
    "\n",
    "$$\n",
    "\\nabla f(0.6,-0.4)= (2(0.6-3), 4(-0.4+1)) = (-4.8, 2.4).\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "(0.6,-0.4) - 0.1(-4.8,2.4) = (1.08,\\,-0.64).\n",
    "$$\n",
    "\n",
    "Value $f(1.08,-0.64)=3.9456$ — decreasing further.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Gradients` is the transformation of `Vector` to `Scalar` fields, providing a way to understand how functions change in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2090",
   "metadata": {},
   "source": [
    "## **Understanding `Jacobian` Chain Rule**\n",
    "\n",
    "**Khan Academy**\n",
    "\n",
    "[PreReq_For_Jacobian](https://www.youtube.com/watch?v=VmfTXVG9S0U)\n",
    "\n",
    "[Understand_Jacobian](https://www.youtube.com/watch?v=wCZ1VEmVjVo)\n",
    "\n",
    "Watch the above video to have complete idea of `Matrix`, `Vectors` and `Jacobian`.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **What is `Jacobian`?**\n",
    "\n",
    "The `Jacobian` is a matrix that contains all the first-order partial derivatives of a vector-valued function. If we have a function $\\mathbf{f} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix is an $m \\times n$ matrix given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Jacobian` is another way of representing `Partial Derivatives` of a function that transforms a `vector input` into a `vector output`.\n",
    "\n",
    "For example, if we've a function `f: R^2 -> R^2` defined as:\n",
    "\n",
    "$$\n",
    "f(x,y) = \\begin{bmatrix}\n",
    "x^2 + y \\\\\n",
    "2x + 3y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix is given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "So, first we take the `Partial Derivative` for the first function i.e. `f_1(x,y) = x^2 + y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_1}{\\partial x} = 2x, \\quad \\frac{\\partial f_1}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "Next, for the second function i.e. `f_2(x,y) = 2x + 3y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_2}{\\partial x} = 2, \\quad \\frac{\\partial f_2}{\\partial y} = 3\n",
    "$$\n",
    "\n",
    "Putting it all together, the Jacobian matrix is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "2x & 1 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Jacobian` is the matrix representing best `Linear Map` approximation of a function `f: R^n -> R^m` at a given point `(a,b)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8fcdb",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9b175",
   "metadata": {},
   "source": [
    "# Jacobian — why we need it, and exactly what information it gives (deep, ordered explanation)\n",
    "\n",
    "## 1) **Why do we find the Jacobian?** (the short motivation)\n",
    "\n",
    "- When a function has **many inputs** and **many outputs** (i.e. $f:\\mathbb{R}^n\\to\\mathbb{R}^m$), a single number cannot describe how the output changes when the input is nudged.\n",
    "- We need a structured object that gives the **first-order** (linear) relationship between every input direction and every output component simultaneously. That object is the **Jacobian matrix** $J_f(x)$.\n",
    "- Practically, the Jacobian lets us:\n",
    "\n",
    "  - **Linearize** a nonlinear map locally (so we can predict small changes cheaply).\n",
    "  - **Propagate sensitivities** through compositions (chain rule — the reason backprop works).\n",
    "  - **Analyze stability** of dynamical systems (eigenvalues of Jacobian).\n",
    "  - **Measure local volume change** (Jacobian determinant) when warping coordinate systems or densities.\n",
    "\n",
    "So: **we compute Jacobians because they are the minimal, exact-to-first-order linear model of a vector-valued function at a point** — and that model is useful for prediction, inversion, optimization, and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) **What information does the Jacobian give?** (granular list, with meaning)\n",
    "\n",
    "Let $f=(f_1,\\dots,f_m):\\mathbb{R}^n\\to\\mathbb{R}^m$. The Jacobian at $x$ is the $m\\times n$ matrix\n",
    "\n",
    "$$\n",
    "J_f(x)=\\begin{bmatrix}\n",
    "\\partial f_1/\\partial x_1 & \\cdots & \\partial f_1/\\partial x_n\\\\[4pt]\n",
    "\\vdots & \\ddots & \\vdots\\\\[4pt]\n",
    "\\partial f_m/\\partial x_1 & \\cdots & \\partial f_m/\\partial x_n\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "What this matrix **tells you**:\n",
    "\n",
    "1. **Local linear approximation (the main fact)**\n",
    "   For small $h\\in\\mathbb{R}^n$,\n",
    "\n",
    "   $$\n",
    "   f(x+h) \\approx f(x) + J_f(x)\\,h.\n",
    "   $$\n",
    "\n",
    "   So $J_f(x)$ is the linear map that predicts the output change from an input perturbation.\n",
    "\n",
    "2. **Rows = per-output gradients**\n",
    "   Row $i$ is $\\nabla f_i(x)^\\top$: it gives how output $f_i$ changes with each input coordinate. If you care about one particular output, read its row.\n",
    "\n",
    "3. **Columns = sensitivities to each input**\n",
    "   Column $j$ tells how a small change in input coordinate $x_j$ simultaneously affects all output components. So column $j$ = “what happens to the whole output vector if I nudge $x_j$”.\n",
    "\n",
    "4. **Directional derivatives (J times a vector)**\n",
    "   For any input direction $v$,\n",
    "\n",
    "   $$\n",
    "   J_f(x)\\,v = \\lim_{t\\to0}\\frac{f(x+t v)-f(x)}{t}.\n",
    "   $$\n",
    "\n",
    "   That is, $J_f(x)\\,v$ is the _vector_ of directional derivatives — it tells the instantaneous rate of change of each output when moving along direction $v$.\n",
    "\n",
    "5. **Chain rule / composition**\n",
    "   If $g:\\mathbb{R}^m\\to\\mathbb{R}^p$, then\n",
    "\n",
    "   $$\n",
    "   J_{g\\circ f}(x) = J_g\\big(f(x)\\big)\\;J_f(x).\n",
    "   $$\n",
    "\n",
    "   Composition reduces to matrix multiplication — this is the algebraic backbone of backpropagation.\n",
    "\n",
    "6. **Determinant = local volume/area scaling (square case)**\n",
    "   If $m=n$, then $|\\det J_f(x)|$ is the approximate factor by which $f$ scales small volumes near $x$. The sign of $\\det$ indicates orientation-preserving or reversing.\n",
    "\n",
    "7. **Rank = local dimension and invertibility**\n",
    "\n",
    "   - $\\mathrm{rank}\\,J_f(x)$ equals the number of independent output directions produced by small input changes.\n",
    "   - If $m=n$ and $\\det J_f(x)\\neq 0$, then $f$ is locally invertible at $x$ (Inverse Function Theorem).\n",
    "   - If rank is less than $\\min(m,n)$, the mapping locally collapses some directions (information loss).\n",
    "\n",
    "8. **Singular values and conditioning**\n",
    "   Singular values of $J_f(x)$ measure how much the map stretches or squashes different orthogonal directions. The ratio (largest/min smallest) is the condition number — it tells how sensitive outputs or inverse computations are to perturbations.\n",
    "\n",
    "9. **Adjoint / transpose and gradients of scalar outputs**\n",
    "   If $g=f$ produces a scalar ($m=1$), the Jacobian is a $1\\times n$ row, and its transpose is the gradient column vector. More generally, for scalar loss $L:\\mathbb{R}^m\\to\\mathbb{R}$,\n",
    "\n",
    "   $$\n",
    "   \\nabla_x (L\\circ f)(x) = J_f(x)^\\top \\,\\nabla_y L\\big(f(x)\\big).\n",
    "   $$\n",
    "\n",
    "   This is how you “pull back” gradients in reverse-mode autodiff.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) **Concrete numeric example — step-by-step (so it’s not abstract)**\n",
    "\n",
    "Take the same example we used earlier:\n",
    "\n",
    "$$\n",
    "f(x,y)=\\begin{bmatrix} x^2 + y \\\\[4pt] 2x + 3y \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "1. Compute Jacobian symbolically:\n",
    "\n",
    "$$\n",
    "J_f(x,y)=\\begin{bmatrix} 2x & 1 \\\\[4pt] 2 & 3 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "2. Evaluate at point $a=(1,-1)$:\n",
    "\n",
    "- $2x|_{x=1}=2$.\n",
    "- So\n",
    "\n",
    "$$\n",
    "J_f(1,-1)=\\begin{bmatrix} 2 & 1 \\\\[4pt] 2 & 3 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "3. Pick a small input perturbation $h=(\\Delta x, \\Delta y)=(0.01,\\,-0.02)$.\n",
    "\n",
    "4. **Linear prediction**: compute $J_f(a)\\,h$.\n",
    "\n",
    "Calculate component-wise (digit-by-digit):\n",
    "\n",
    "- First component: $2\\cdot 0.01 + 1\\cdot(-0.02) = 0.02 - 0.02 = 0.00.$\n",
    "- Second component: $2\\cdot 0.01 + 3\\cdot(-0.02) = 0.02 - 0.06 = -0.04.$\n",
    "\n",
    "So the Jacobian predicts the output change $\\Delta f_{\\text{lin}}=(0.00,\\,-0.04)$.\n",
    "\n",
    "5. **Exact change**: compute $f(a+h)-f(a)$.\n",
    "\n",
    "- New input: $x_{\\text{new}} = 1 + 0.01 = 1.01,\\quad y_{\\text{new}} = -1 - 0.02 = -1.02.$\n",
    "- First component at new point: $x_{\\text{new}}^2 + y_{\\text{new}} = 1.01^2 - 1.02$.\n",
    "\n",
    "  - $1.01^2 = 1.0201$. So first component = $1.0201 - 1.02 = 0.0001.$\n",
    "  - Old first component: $1^2 + (-1) = 0.$ So exact first change $= 0.0001.$\n",
    "\n",
    "- Second component at new point: $2\\cdot 1.01 + 3\\cdot(-1.02) = 2.02 - 3.06 = -1.04.$\n",
    "\n",
    "  - Old second component: $2\\cdot1 + 3\\cdot(-1) = -1.$ So exact second change $= -1.04 - (-1) = -0.04.$\n",
    "\n",
    "Thus $\\Delta f_{\\text{exact}} = (0.0001,\\,-0.04)$.\n",
    "\n",
    "6. **Error of linear approximation**:\n",
    "\n",
    "- Error vector $= \\Delta f_{\\text{exact}} - \\Delta f_{\\text{lin}} = (0.0001,\\;0).$\n",
    "- Norm of $h$: $\\|h\\| = \\sqrt{0.01^2 + (-0.02)^2} = \\sqrt{0.0001 + 0.0004} = \\sqrt{0.0005} \\approx 0.02236067977.$\n",
    "- Norm of error $\\approx 0.0001.$\n",
    "- Relative error $\\frac{\\|error\\|}{\\|h\\|} \\approx \\frac{0.0001}{0.02236068} \\approx 0.00447$. That goes to 0 as we shrink $h$. The Jacobian captured the full linear part; only higher-order terms (here $(\\Delta x)^2$) remain.\n",
    "\n",
    "7. **Determinant & invertibility check at this point**\n",
    "\n",
    "- Compute $\\det J_f(1,-1) = 2\\cdot 3 - 1\\cdot 2 = 6 - 2 = 4.$\n",
    "- Because $\\det\\neq 0$, the map is locally invertible at $a$ and small input-area elements are scaled by a factor $\\approx 4$ in output space.\n",
    "\n",
    "This numeric worked example makes concrete all the conceptual items above: rows/columns, J×h = directional derivative vector, determinant = area scaling, and why the Jacobian is the _best_ linear predictor.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) **Chain rule & backprop (practical algebraic uses)**\n",
    "\n",
    "- For compositions $h(x)=g(f(x))$:\n",
    "\n",
    "  $$\n",
    "  J_h(x)=J_g(f(x))\\,J_f(x).\n",
    "  $$\n",
    "\n",
    "  - In **forward mode** autodiff you compute $J_f(x)\\,v$ for a direction $v$ (Jacobian-vector product, JVP).\n",
    "  - In **reverse mode** autodiff (backprop) you compute $J_f(x)^\\top w$ for a row/adjoint $w$ (vector-Jacobian product, VJP). For scalar loss $L$, $\\nabla_x (L\\circ f) = J_f(x)^\\top \\nabla_y L(f(x))$.\n",
    "\n",
    "This algebraic property is exactly why neural network training is efficient: instead of computing full Jacobians, autodiff calculates Jacobian-vector or vector-Jacobian products cheaply.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) **Geometric intuition (circle → ellipse)**\n",
    "\n",
    "Place a tiny circle of input perturbations around $x$. Under $f$, that circle maps approximately to an ellipse in output space. The Jacobian is the linear map that sends the circle to that ellipse:\n",
    "\n",
    "- The ellipse axes lengths = singular values of $J$.\n",
    "- The directions of axes = singular vectors.\n",
    "- The area of the ellipse ≈ $|\\det J|$ × (area of circle).\n",
    "\n",
    "So Jacobian = “how the map stretches and rotates infinitesimal neighborhoods.”\n",
    "\n",
    "---\n",
    "\n",
    "## 6) **When & where Jacobians are used (practical examples)**\n",
    "\n",
    "- **Numerical solvers**: Newton’s method for systems uses the Jacobian to linearize $F(x)=0$.\n",
    "- **Stability / dynamical systems**: eigenvalues of the Jacobian tell whether small perturbations grow/decay.\n",
    "- **Backprop / deep learning**: propagate gradients via Jacobian transpose or efficient VJP/JVP operations.\n",
    "- **Normalizing flows** (generative models): log-density uses $\\log|\\det J|$ to account for volume change.\n",
    "- **Computer vision / neuroimaging**: deformation fields between images — Jacobian determinant indicates local compression/expansion (useful in voxel-based morphometry).\n",
    "- **Sensitivity analysis**: find which inputs most affect outputs by inspecting columns or computing norms.\n",
    "- **Inverse function / implicit function theorems**: Jacobian non-singularity is the condition for local solvability/invertibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) **Quick hierarchy recap (how Jacobian fits with derivative / gradient / Hessian)**\n",
    "\n",
    "1. **1-D derivative**: scalar $f'(a)$ — slope of tangent, linear approximation.\n",
    "2. **Gradient**: $\\nabla f(x)$ for scalar-valued multivariable functions — vector of partial derivatives; best linear predictor for scalar output.\n",
    "3. **Jacobian**: $J_f(x)$ for vector-valued functions — matrix bundling all partial derivatives; best linear predictor for vector outputs.\n",
    "4. **Hessian**: matrix of second derivatives for scalar functions — describes curvature (quadratic term in Taylor expansion).\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Wrap-up / next steps\n",
    "\n",
    "Would you like any of these next actions?\n",
    "\n",
    "- I can **graph** the example $f(x,y)$ and show the circle→ellipse mapping visually.\n",
    "- I can produce a **small Python notebook** that numerically verifies $J_f(x)h$ vs exact changes for several $h$ (and plots relative error as $\\|h\\|\\to 0$).\n",
    "- Or I can show a worked **chain-rule example** across two composed functions (with algebra and numbers) to illustrate backprop.\n",
    "\n",
    "Which one should I show? (If you want the notebook/plot, say “notebook” and I’ll generate it.)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
