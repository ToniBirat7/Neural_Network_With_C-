{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd346528",
   "metadata": {},
   "source": [
    "# **Resources**\n",
    "\n",
    "## **Neural Networks**\n",
    "\n",
    "### **Complete Mathematics of Neural Network**\n",
    "\n",
    "[Adam_Dhalla_Best](https://www.youtube.com/watch?v=Ixl3nykKG9M&t=2s)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **Mathematics**\n",
    "\n",
    "### **Multivariable Calculus**\n",
    "\n",
    "[IIT_Rookie](https://www.youtube.com/playlist?list=PL1XTxGlLddCzE1mHJVul22iGYE5NrSB3I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b64c71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d59cf",
   "metadata": {},
   "source": [
    "# **Complete Mathematics of Neural Network**\n",
    "\n",
    "**course Syllabus**\n",
    "\n",
    "<img src='./Notes_Images/syllabus.png'>\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "- Basics of Linear Algebra\n",
    "\n",
    "- Multivariable Calculus\n",
    "\n",
    "  - Differential Equations\n",
    "\n",
    "  - Jacobian / Gradients\n",
    "\n",
    "- Base ML Knowledge\n",
    "\n",
    "### **Agenda**\n",
    "\n",
    "- **Big Picture of Neural Networks**\n",
    "\n",
    "- **Multivariable Calculus Refresher**\n",
    "\n",
    "- **Neuron as Function**\n",
    "\n",
    "- **Jacobians and Neural Networks**\n",
    "\n",
    "- **Gradient Descent**\n",
    "\n",
    "- **Backpropagation**\n",
    "\n",
    "- **Backpropagation as Matrix Multiplication**\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "<img src='./Notes_Images/notation.png'>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a22048",
   "metadata": {},
   "source": [
    "## **Neural Networks as Functions**\n",
    "\n",
    "Neural Network is a big fancy function that is made up of many smaller functions (neurons) that work together to transform input data into output predictions.\n",
    "\n",
    "All the parameters to this functions are the weights and biases of the neurons, which are learned during training.\n",
    "\n",
    "Input to this function is a `vector` of features, and the output is a `vector` of predictions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Neural Network` is just a `Big Calculus Problem` in which we are trying to minimize the difference between the predicted output and the actual output (loss function) by adjusting the weights and biases using optimization techniques like gradient descent.\n",
    "\n",
    "It's all about finding the `Derivative` of `Loss Function` w.r.t `Weights` and `Biases` (parameters of the model) and using that information to update the parameters in the direction that reduces the loss.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ L $ is the loss function\n",
    "\n",
    "- $ \\hat{y} $ is the predicted output\n",
    "\n",
    "- $ w $ is the weight\n",
    "\n",
    "This is just the chain rule from calculus, and it allows us to compute the gradient of the loss function with respect to the weights and biases of the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Say we've a function with thousands parameters, and to figure out how to lower that loss we'll have to find out how much each `Weight` and `Bias` contributes to the loss. So that we can add or substract some value to our initial `Weights` to lower the `Loss`.\n",
    "\n",
    "This is done with `Backpropagation` and `Gradient Descent`.\n",
    "\n",
    "This means we need to find $ \\frac{\\partial L}{\\partial w_i} $ for each `Weight` $ w_i $ in the model.\n",
    "\n",
    "This is where automatic differentiation comes in handy. It allows us to compute these gradients efficiently without having to derive them manually.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## **Multivariable Calculus Refresher**\n",
    "\n",
    "### **Understand the Meaning of `Gradients`**\n",
    "\n",
    "In `Univariate Calculus`, the derivative of a function at a point gives us the `slope of the tangent line to the function at that point`. This tells us how much the function is changing at that point.\n",
    "\n",
    "For example, let's understand mathematically:\n",
    "\n",
    "Let $f(x) = x^2$. The derivative of this function is $f'(x) = 2x$. This tells us that at any point $x$, the slope of the tangent line to the function is $2x$. If we evaluate this at $x=1$, we find that the slope is $2$. This means that the function is increasing at that point.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In `Multivariable Calculus`, we extend this concept to functions of multiple variables. The gradient of a function is a `vector` that contains all of its `partial derivatives`. It points in the `direction` of the `steepest ascent` of the function.\n",
    "\n",
    "Mathematically, if we have a function $f(x, y)$, the gradient is given by:\n",
    "\n",
    "$$ \\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) $$\n",
    "\n",
    "This vector points in the direction of the steepest increase of the function.\n",
    "\n",
    "**Image Notes**\n",
    "\n",
    "<img src='./Notes_Images/note1.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note2.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note3.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<img src='./Notes_Images/note4.png'>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Worked Examples to Understand `Gradient` and it's Significance**\n",
    "\n",
    "**Example A — linear approximation and directional derivative**\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "f(x,y) = x^2 + 4y^2.\n",
    "$$\n",
    "\n",
    "Compute $\\nabla f(x,y) = (2x, 8y)$. At point $(1,1)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(1,1) = (2,8).\n",
    "$$\n",
    "\n",
    "Magnitude:\n",
    "\n",
    "$$\n",
    "\\|\\nabla f\\| = \\sqrt{2^2 + 8^2} = \\sqrt{4 + 64} = \\sqrt{68}.\n",
    "$$\n",
    "\n",
    "We can keep the exact $\\sqrt{68}$ or approximate: $\\sqrt{68}\\approx 8.246211\\ldots$ — so the steepest rate of increase is about $8.246$ units of $f$ per unit distance in $(x,y)$-space.\n",
    "\n",
    "If we move $\\Delta\\mathbf{x}=(0.01,0.005)$, linear prediction:\n",
    "\n",
    "$$\n",
    "\\Delta f \\approx \\nabla f\\cdot \\Delta\\mathbf{x} = 2\\cdot 0.01 + 8\\cdot 0.005 = 0.02 + 0.04 = 0.06.\n",
    "$$\n",
    "\n",
    "Actual $f(1.01,1.005)$ equals\n",
    "\n",
    "$$\n",
    "(1.01)^2 + 4(1.005)^2 = 1.0201 + 4\\cdot 1.010025 = 1.0201 + 4.0401 = 5.0602,\n",
    "$$\n",
    "\n",
    "initial $f(1,1)=5$. Actual change $=0.0602$, close to linear prediction $0.06$. This demonstrates the gradient gives a very good first-order prediction.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Example B — gradient descent in 2D (one optimization step)**\n",
    "\n",
    "Minimize\n",
    "\n",
    "$$\n",
    "f(x,y)=(x-3)^2 + 2(y+1)^2.\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "\n",
    "$$\n",
    "\\nabla f = (2(x-3),\\; 4(y+1)).\n",
    "$$\n",
    "\n",
    "Start at $(0,0)$. Compute gradient at $(0,0)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(0,0) = (2(0-3), 4(0+1)) = (-6, 4).\n",
    "$$\n",
    "\n",
    "Use learning rate $\\eta=0.1$. Gradient descent update:\n",
    "\n",
    "$$\n",
    "(x_{\\text{new}},y_{\\text{new}}) = (0,0) - 0.1\\cdot(-6,4) = (0+0.6,\\; 0-0.4) = (0.6,\\,-0.4).\n",
    "$$\n",
    "\n",
    "Function values:\n",
    "\n",
    "$$\n",
    "f(0,0) = 9 + 2 = 11.\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(0.6,-0.4) = (0.6-3)^2 + 2(-0.4+1)^2 = (-2.4)^2 + 2(0.6)^2 = 5.76 + 2\\cdot0.36 = 5.76 + 0.72 = 6.48.\n",
    "$$\n",
    "\n",
    "Big drop from $11$ to $6.48$ in one step — the gradient told us a good descent direction.\n",
    "\n",
    "Second step (quick):\n",
    "\n",
    "$$\n",
    "\\nabla f(0.6,-0.4)= (2(0.6-3), 4(-0.4+1)) = (-4.8, 2.4).\n",
    "$$\n",
    "\n",
    "Update:\n",
    "\n",
    "$$\n",
    "(0.6,-0.4) - 0.1(-4.8,2.4) = (1.08,\\,-0.64).\n",
    "$$\n",
    "\n",
    "Value $f(1.08,-0.64)=3.9456$ — decreasing further.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Gradients` is the transformation of `Vector` to `Scalar` fields, providing a way to understand how functions change in multiple dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe2090",
   "metadata": {},
   "source": [
    "## **Understanding `Jacobian` Chain Rule**\n",
    "\n",
    "[Understand_Jacobian](https://www.youtube.com/watch?v=wCZ1VEmVjVo)\n",
    "\n",
    "Watch the above video to have complete idea of `Matrix`, `Vectors` and `Jacobian`.\n",
    "\n",
    "### **What is `Jacobian`?**\n",
    "\n",
    "The `Jacobian` is a matrix that contains all the first-order partial derivatives of a vector-valued function. If we have a function $\\mathbf{f} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix is an $m \\times n$ matrix given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Jacobian` is another way of representing `Partial Derivatives` of a function that transforms a `vector input` into a `vector output`.\n",
    "\n",
    "For example, if we've a function `f: R^2 -> R^2` defined as:\n",
    "\n",
    "$$\n",
    "f(x,y) = \\begin{bmatrix}\n",
    "x^2 + y \\\\\n",
    "2x + 3y\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix is given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $f_i$ are the components of the vector-valued function $\\mathbf{f}$ and $x_j$ are the components of the input vector $\\mathbf{x}$.\n",
    "\n",
    "So, first we take the `Partial Derivative` for the first function i.e. `f_1(x,y) = x^2 + y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_1}{\\partial x} = 2x, \\quad \\frac{\\partial f_1}{\\partial y} = 1\n",
    "$$\n",
    "\n",
    "Next, for the second function i.e. `f_2(x,y) = 2x + 3y`:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f_2}{\\partial x} = 2, \\quad \\frac{\\partial f_2}{\\partial y} = 3\n",
    "$$\n",
    "\n",
    "Putting it all together, the Jacobian matrix is:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\begin{bmatrix}\n",
    "2x & 1 \\\\\n",
    "2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Summary**\n",
    "\n",
    "`Jacobian` is the matrix representing best `Linear Map` approximation of a function `f: R^n -> R^m` at a given point `(a,b)`.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
