{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd346528",
   "metadata": {},
   "source": [
    "## **Resources**\n",
    "\n",
    "### **Complete Mathematics of Neural Network**\n",
    "\n",
    "[Adam_Dhalla_Best](https://www.youtube.com/watch?v=Ixl3nykKG9M&t=2s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d59cf",
   "metadata": {},
   "source": [
    "# **Complete Mathematics of Neural Network**\n",
    "\n",
    "**course Syllabus**\n",
    "\n",
    "<img src='./Notes_Images/syllabus.png'>\n",
    "\n",
    "### **Prerequisites**\n",
    "\n",
    "- Basics of Linear Algebra\n",
    "\n",
    "- Multivariable Calculus\n",
    "\n",
    "  - Differential Equations\n",
    "\n",
    "  - Jacobian / Gradients\n",
    "\n",
    "- Base ML Knowledge\n",
    "\n",
    "### **Agenda**\n",
    "\n",
    "- **Big Picture of Neural Networks**\n",
    "\n",
    "- **Multivariable Calculus Refresher**\n",
    "\n",
    "- **Neuron as Function**\n",
    "\n",
    "- **Jacobians and Neural Networks**\n",
    "\n",
    "- **Gradient Descent**\n",
    "\n",
    "- **Backpropagation**\n",
    "\n",
    "- **Backpropagation as Matrix Multiplication**\n",
    "\n",
    "<hr>\n",
    "\n",
    "### **Notation**\n",
    "\n",
    "<img src='./Notes_Images/notation.png'>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a22048",
   "metadata": {},
   "source": [
    "## **Neural Networks as Functions**\n",
    "\n",
    "Neural Network is a big fancy function that is made up of many smaller functions (neurons) that work together to transform input data into output predictions.\n",
    "\n",
    "All the parameters to this functions are the weights and biases of the neurons, which are learned during training.\n",
    "\n",
    "Input to this function is a `vector` of features, and the output is a `vector` of predictions.\n",
    "\n",
    "<hr>\n",
    "\n",
    "`Neural Network` is just a `Big Calculus Problem` in which we are trying to minimize the difference between the predicted output and the actual output (loss function) by adjusting the weights and biases using optimization techniques like gradient descent.\n",
    "\n",
    "It's all about finding the `Derivative` of `Loss Function` w.r.t `Weights` and `Biases` (parameters of the model) and using that information to update the parameters in the direction that reduces the loss.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial w} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ L $ is the loss function\n",
    "\n",
    "- $ \\hat{y} $ is the predicted output\n",
    "\n",
    "- $ w $ is the weight\n",
    "\n",
    "This is just the chain rule from calculus, and it allows us to compute the gradient of the loss function with respect to the weights and biases of the model.\n",
    "\n",
    "<hr>\n",
    "\n",
    "Say we've a function with thousands parameters, and to figure out how to lower that loss we'll have to find out how much each `Weight` and `Bias` contributes to the loss. So that we can add or substract some value to our initial `Weights` to lower the `Loss`.\n",
    "\n",
    "This is done with `Backpropagation` and `Gradient Descent`.\n",
    "\n",
    "This means we need to find $ \\frac{\\partial L}{\\partial w_i} $ for each `Weight` $ w_i $ in the model.\n",
    "\n",
    "This is where automatic differentiation comes in handy. It allows us to compute these gradients efficiently without having to derive them manually.\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## **Multivariable Calculus Refresher**\n",
    "\n",
    "### **Understand the Meaning of `Gradients`**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
